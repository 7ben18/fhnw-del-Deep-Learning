{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Challenge 1\n",
    "\n",
    "Modul: Deep Learning  \n",
    "Thema: Hyperparameter und Model Tuning  \n",
    "Student: Si Ben Tran  \n",
    "Fachexperte: Martin Melchior  \n",
    "Abgabetermin: Ende FS23   \n",
    "\n",
    "Ziel:  \n",
    "Implementiere, trainiere und validiere ein Neuronales Netzwerk Modell für einen\n",
    "Klassifikationstask mit geeignetem Dataset. Der zu modellierende Task und die dazu\n",
    "passenden Daten sollen am Anfang der Mini-Challenge individuell vereinbart werden. Dabei\n",
    "können die Studierenden auch ihre Wünsche einbringen und Task/Datensatz vorschlagen.\n",
    "Dabei ist aber darauf zu achten, dass der Fokus auf das Training und die Evaluation gelegt\n",
    "und nicht zu viel Zeit mit Datenbeschaffung, -Analyse und -Vorverarbeitung verbraucht wird.\n",
    "Der Datensatz sollte auch nicht zu gross sein, um viele verschiedene Modell- und\n",
    "Parametervarianten evaluieren zu können. Ausserdem sollten nicht zu ausgeklügelte\n",
    "komplexe Modell-Architekturen untersucht werden. Ein MLP für die Klassifikation allenfalls\n",
    "mit ein paar CNN-Layern für Feature-Extraktion sollte ausreichen.\n",
    "\n",
    "Zeitlicher Rahmen:  \n",
    "Wird beim Schritt 1 verbindlich festgelegt.\n",
    "\n",
    "Beurteilung:  \n",
    "Beurteilt wird auf Basis des abgegebenen Notebooks:  \n",
    "• Vollständige und korrekte Umsetzung der vereinbarten Aufgabestellung.  \n",
    "• Klare, gut-strukturierte Umsetzung.  \n",
    "• Schlüssige Beschreibung und Interpretation der Ergebnisse. Gut gewählte und gut kommentierten Plots und Tabellen.  \n",
    "• Vernünftiger Umgang mit (Computing-)Ressourcen.  \n",
    "• Verständliche Präsentation der Ergebnisse.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbeitsschritte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 1: Auswahl Task / Datensatz  \n",
    "\n",
    "**Absprache/Beschluss** mit Fachcoach über Daten/Task.\n",
    "\n",
    "**1. Mache Dir Gedanken, mit welchen Daten Du arbeiten möchtest und welcher Task\n",
    "gelernt werden soll.**  \n",
    "\n",
    "Gedanken: Ich moechte gerne ein CNN-Modell zur Klassifizierung des CIFAR-10-Datensatzes zu trainieren, um ein besseres Verständnis von CNNs zu erlangen und die Hyperparameter mithilfe von Weights & Biases-Integration zu optimieren.\n",
    "\n",
    "**2. Diskutiere die Idee mit dem Fachcoach.**  \n",
    "\n",
    "Diskussion: Es wurde mal vorerst vereinbart, am 16.04.23 die Minichallenge 1 abzugeben und die Aufgaben zu erledigen. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 2: Daten Kennenlernen\n",
    "Wie erwähnt, sollte dieser Schritt nicht viel Zeit beanspruchen, da der Datensatz einfach\n",
    "sein soll!\n",
    "\n",
    "\n",
    "**1. Mache Dich mit dem Datensatz vertraut, indem Du eine (kurze) explorative Analyse\n",
    "der Features durchführst: z.B. Vergleich der Klassen pro Feature, Balanciertheit der\n",
    "Klassen.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Laden\n",
    "\n",
    "## PyTorch Libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn \n",
    "# import torch.nn.functional as F for activation functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import pytorch optimizer for training\n",
    "import torch.optim as optim\n",
    "\n",
    "# import summary function from torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "## Data Science Libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "## import wandb for logging and tracking\n",
    "import wandb\n",
    "\n",
    "# Device Einstellungen\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('torch cuda is available:', torch.cuda.is_available())\n",
    "print('Using device:', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "\n",
    "# wandb login \n",
    "print(\"Successfull wandb login: \", wandb.login())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for getting cifar10 dataset\n",
    "def get_dataset(data_dir = './data', dimension_info = True):\n",
    "    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True,\n",
    "                                            download=True, transform=transforms.ToTensor())\n",
    "    valset = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "                                             download=True, transform=transforms.ToTensor())\n",
    "    if dimension_info:\n",
    "        print(\"Trainingsset: Anzahl Bilder {}, Hoehe {}, Breite {}, Farbkanäle {}\".format(trainset.data.shape[0], trainset.data.shape[1], trainset.data.shape[2], trainset.data.shape[3]))\n",
    "        print(\"Valset: Anzahl Bilder {} , Hoehe {}, Breite {}, Farbkanäle {}\".format(valset.data.shape[0], valset.data.shape[1], valset.data.shape[2], valset.data.shape[3]))\n",
    "\n",
    "    return trainset, valset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "trainset, valset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for plotting 10x10 grid\n",
    "def plot_10x10_grid(data, title):\n",
    "    '''\n",
    "    plotting 10x10 grid of images\n",
    "    '''\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(15, 15))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # plot image \n",
    "            ax[i, j].imshow(data.data[i*10+j])\n",
    "            # add title to each subplot\n",
    "            ax[i, j].set_title(data.classes[data.targets[i*10+j]])\n",
    "            # set axis labeling off\n",
    "            ax[i, j].axis('off')\n",
    "            # set grid off\n",
    "            ax[i, j].grid(False)\n",
    "    # add title and adjust layout\n",
    "    plt.suptitle(title, fontsize=20, y = 1.005)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot testset 10x10 grid\n",
    "plot_10x10_grid(trainset, 'Trainset CIFAR-10')\n",
    "plot_10x10_grid(valset, 'Valset CIFAR-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung Anzahl Bilder Pro Klasse  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# plot barplot\n",
    "sns.barplot(x=trainset.classes, y=np.bincount(trainset.targets), ax=ax1)\n",
    "sns.barplot(x=valset.classes, y=np.bincount(valset.targets), ax=ax2)\n",
    "# set title and labels\n",
    "ax1.set_title(\"CIFAR-10 Trainset\", fontsize=15)\n",
    "ax2.set_title(\"CIFAR-10 Valset\", fontsize=15)\n",
    "ax1.set_ylabel(\"Anzahl Bilder\", fontsize=10)\n",
    "ax1.set_xlabel(\"Klassen\", fontsize=10)\n",
    "ax2.set_ylabel(\"Anzahl Bilder\", fontsize=10)\n",
    "ax2.set_xlabel(\"Klassen\", fontsize=10)\n",
    "# rotate xticklabels\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
    "# add suptitle\n",
    "fig.suptitle(\"Anzahl Bilder pro Klasse in Train- und Valset\", fontsize=20, y = 1.02)\n",
    "# add number of images to barplot\n",
    "for p in ax1.patches:\n",
    "    ax1.annotate(str(p.get_height()), (p.get_x(), p.get_height()))\n",
    "for p in ax2.patches:\n",
    "    ax2.annotate(str(p.get_height()), (p.get_x(), p.get_height()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of the colour channels\n",
    "def get_channel_distribution(data):\n",
    "    '''\n",
    "    get the distribution of the colour channels\n",
    "    '''\n",
    "    # create empty list for each channel\n",
    "    r = []\n",
    "    g = []\n",
    "    b = []\n",
    "    # calculate the mean for each \n",
    "    for i in range(data.shape[0]):\n",
    "        r.append(np.mean(data[i, :, :, 0]))\n",
    "        g.append(np.mean(data[i, :, :, 1]))\n",
    "        b.append(np.mean(data[i, :, :, 2]))\n",
    "    return r, g, b\n",
    "\n",
    "# get the distribution of the colour channels\n",
    "r_train, g_train, b_train = get_channel_distribution(trainset.data)\n",
    "r_test, g_test, b_test = get_channel_distribution(valset.data)\n",
    "\n",
    "# plot the distribution of the colour channels\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.histplot(r_train, ax=ax1, color='r', alpha = 0.5)\n",
    "sns.histplot(g_train, ax=ax1, color='g', alpha = 0.5)\n",
    "sns.histplot(b_train, ax=ax1, color='b', alpha = 0.5)\n",
    "sns.histplot(r_test, ax=ax2, color='r', alpha = 0.5)\n",
    "sns.histplot(g_test, ax=ax2, color='g', alpha = 0.5)\n",
    "sns.histplot(b_test, ax=ax2, color='b', alpha = 0.5)\n",
    "ax1.set_title(\"CIFAR-10 Trainset, Farbkanalverteilung\", fontsize=15)\n",
    "ax2.set_title(\"CIFAR-10 Testset, Farbkanalverteilung\", fontsize=15)\n",
    "ax1.set_ylabel(\"Häufigkeit\", fontsize=10)\n",
    "ax1.set_xlabel(\"Farbwert\", fontsize=10)\n",
    "ax2.set_ylabel(\"Häufigkeit\", fontsize=10)\n",
    "ax2.set_xlabel(\"Farbwert\", fontsize=10)\n",
    "fig.suptitle(\"Farbkanalverteilung\", fontsize=20, y = 1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Führe ein geeignetes Preprocessing durch, z.B. Normalisierung der Daten.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(normalize=True, batch_size=64, data_dir='./data', num_workers=2, pin_memory=True, drop_last=True):\n",
    "    '''\n",
    "    get train and validation dataloaders for a given dataset\n",
    "\n",
    "    dataset: the name of the dataset to be used, currently supports CIFAR10 and MNIST\n",
    "    normalize: a boolean indicating whether to apply normalization to the data\n",
    "    batch_size: the batch size for loading data\n",
    "    data_dir: the directory where the data will be downloaded and stored\n",
    "    num_workers: the number of worker processes for loading data\n",
    "    pin_memory: a boolean indicating whether to pin the memory for faster data transfer\n",
    "    drop_last: a boolean indicating whether to drop the last incomplete batch when the dataset size is not divisible by the batch size\n",
    "    '''\n",
    "    # define transforms\n",
    "    if normalize:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # define train and validation datasets\n",
    "    trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    # define train and validation dataloaders\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n",
    "\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and validation dataloaders for CIFAR10 dataset with normalization and batch size of 64\n",
    "trainloader, valloader = get_dataloader(normalize=True, batch_size=5, data_dir='./data', num_workers=2, pin_memory=True, drop_last=True)\n",
    "\n",
    "# dimension of trainloader and valloader batches\n",
    "print(f\"Trainloader batch size: {trainloader.batch_size}\")\n",
    "print(f\"Valloader batch size: {valloader.batch_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Schritt 3: Aufbau Modellierung\n",
    "\n",
    "**Absprache/Beschluss** mit Fachcoach über die zu untersuchenden Varianten (Schritte 4)\n",
    "\n",
    "**1. Lege fest, wie (mit welchen Metriken) Du Modelle evaluieren möchtest.\n",
    "Berücksichtige auch den Fehler in der Schätzung dieser Metriken.**\n",
    "\n",
    "Als Evaluierungsmetrik verwende ich die Accuracy, da es sich um ein Klassifikationsproblem handelt und die Verteilungen der Klassen ueber den ganzen Datensatz hinweg gleichmaessig verteilt sind, sowohl im Trainingsdatensatz als auch im Testdatensatz.\n",
    "\n",
    "Die accuracy berechnet sich wie folgt:\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{Anzahl korrekt klassifiziert}}{\\text{Anzahl aller Beispiele}}\n",
    "$$\n",
    "\n",
    "Als Loss Funktion nutzen wir die Cross-Entropy Loss, da es sich um ein multiple Klassifikationsproblem handelt. Die Cross-Entropy Loss berechnet sich wie folgt:\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "\n",
    "Wir erstellen verschiedene Modelle und Untersuchung dann das beste Modell.\n",
    "Folgend werden die Modelle aufgelistet und beschrieben:  \n",
    "\n",
    "Modell 1: Multi-Layer Perceptron (MLP) mit 2 Hidden Layers und der Aktivierungsfunktion ReLU.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Implementiere Basisfunktionalität, um Modelle zu trainieren und gegeneinander zu\n",
    "evaluieren.**\n",
    "\n",
    "Empfehlenswert ist die Verwendung einer geeigneten MLOps Plattform (z.B. [W&B](https://wandb.ai/site))\n",
    "\n",
    "Hier werden einige wichtige Begriffe definiert bevor wir mit der Implementierung des Modells beginnnen:\n",
    "\n",
    "**Batch Size**   \n",
    "Die Batch Size definiert die Anzahl Bilder, die gleichzeitig in das Modell gegeben werden. \n",
    "Bsp. hat unser Cifar10 Datensatz 50000 Trainingsbilder, eine batchsize = 4 bedeutet, dass wir 4 Bilder gleichzeitig in das Modell geben und gesammthaft 12500 batches haben. (50000/4 = 12500)\n",
    "\n",
    "**Epoche**  \n",
    "Eine Epoche ist ein Durchlauf durch den gesamten Trainingsdatensatz. Bsp. hat unser Cifar10 Datensatz 50000 Trainingsbilder, eine batchsize = 4 bedeutet, dass wir 4 Bilder gleichzeitig in das Modell geben und gesammthaft 12500 batches haben. (50000/4 = 12500). Wenn wir nun 10 Epochen trainieren, dann haben wir 12500 * 10 = 125000 Bilder trainiert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1(nn.Module):\n",
    "    '''\n",
    "    Multi Layer Perceptron 4 Layers\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.fc1 = nn.Linear(32*32*3, 256)\n",
    "        # first hidden layer\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        # output layer\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # input layer\n",
    "        x = self.fc1(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # first hidden layer\n",
    "        x = self.fc2(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # second hidden layer\n",
    "        x = self.fc3(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # output layer\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# create mlp model with 4 layers\n",
    "model_mlp_1 = MLP_1().to(device)\n",
    "\n",
    "# print model architecture\n",
    "print(\"MLP with 4 Layers\")\n",
    "summary(model_mlp_1, (3, 32, 32))\n",
    "\n",
    "class MLP_2(nn.Module):\n",
    "    '''\n",
    "    Multi Layer Perceptron with 6 Layers\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        # first hidden layer\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # second hidden layer\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        # third hidden layer\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        # fourth hidden layer\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        # output layer\n",
    "        self.fc6 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # input layer\n",
    "        x = self.fc1(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # first hidden layer\n",
    "        x = self.fc2(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # second hidden layer\n",
    "        x = self.fc3(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # third hidden layer\n",
    "        x = self.fc4(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # fourth hidden layer\n",
    "        x = self.fc5(x)\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "        # output layer\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "# crete mlp model with 6 layers\n",
    "model_mlp_2 = MLP_2().to(device)\n",
    "\n",
    "# print model architecture\n",
    "print(\"MLP with 6 Layers\")\n",
    "summary(model_mlp_2, (3, 32, 32))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convuliutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1(nn.Module):\n",
    "    '''\n",
    "    Convolutional Neural Network with 2 Convolutional Layers and 3 Fully Connected Layers\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # 2x2 max pooling\n",
    "        # second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        # first fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
    "        # second fully connected layer\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        # third fully connected layer\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        ## second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        ## flatten\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        ## first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        ## second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        ## third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# creating model objects and get the number of parameters\n",
    "model_cnn_1 = CNN_1().to(device)\n",
    "\n",
    "# print the model architecture\n",
    "print(\"CNN with 2 Convolutional Layers and 3 Fully Connected Layers\")\n",
    "summary(model_cnn_1, (3, 32, 32))\n",
    "\n",
    "class CNN_2(nn.Module):\n",
    "    '''\n",
    "    Convolutional Neural Network with 3 Convolutional Layers and 3 Fully Connected Layers\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, padding=3)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # 2x2 max pooling\n",
    "        # second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, padding=3)\n",
    "        # third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=3)\n",
    "        # first fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=32*5*5, out_features=120)\n",
    "        # second fully connected layer\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        # third fully connected layer\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        ## second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        ## third convolutional layer\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        ## flatten\n",
    "        x = x.view(-1, 32*5*5)\n",
    "        ## first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        ## second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        ## third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "# creating model objects and get the number of parameters\n",
    "model_cnn_2 = CNN_2().to(device)\n",
    "\n",
    "# print the model architecture\n",
    "print(\"CNN with 3 Convolutional Layers and 3 Fully Connected Layers\")\n",
    "summary(model_cnn_2, (3, 32, 32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a train function for training the model, parameters are: criterion, optimzer, model and dataloader \n",
    "def train_modell(model, dataloader, criterion, optimizer, num_epochs=1, verbose=False):\n",
    "    '''\n",
    "    This function trains a given model for a given number of epochs and returns the loss and accuracy for each epoch\n",
    "\n",
    "    model: example: model_cnn_1\n",
    "    dataloader: trainloader or valloader\n",
    "    criterion: nn.CrossEntropyLoss()\n",
    "    optimzer: optim.SGD(model.parameters(), lr = )\n",
    "    '''\n",
    "    num_of_total_batches = len(dataloader)\n",
    "    num_of_total_samples = len(dataloader.dataset)\n",
    "    epoch_loss = 0\n",
    "    epoch_correct_predicted = 0\n",
    "\n",
    "    # iteriere durch alle batches im dataloader\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        # images has dimension [batch_size, colour channel, width, height]\n",
    "        # labels is a tensor with (list) with the class number\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward Propagation\n",
    "        labels_predict = model(images)\n",
    "\n",
    "        # calculate the loss based on criterion\n",
    "        loss = criterion(labels_predict, labels)\n",
    "\n",
    "        # Backward Propagation \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate metric loss and accuracy\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        epoch_correct_predicted = epoch_correct_predicted + (labels_predict.argmax(axis=1) == labels).type(torch.float).sum().item()\n",
    "    \n",
    "    # calculate the average loss and accuracy for the epoch\n",
    "    epoch_avg_loss = epoch_loss / num_of_total_batches\n",
    "    epoch_accuracy = epoch_correct_predicted / num_of_total_samples\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Train Loss: {epoch_avg_loss}, Train Accuracy: {epoch_accuracy}\")\n",
    "        \n",
    "    return epoch_avg_loss, epoch_accuracy\n",
    "    \n",
    "# create a test function for testing the model, parameters are: criterion, model and dataloader\n",
    "def test_model(model, dataloader, criterion, verbose=False):\n",
    "    '''\n",
    "    This function tests a given model and returns the loss and accuracy \n",
    "\n",
    "    model: example: model_cnn_1\n",
    "    dataloader: trainloader or valloader\n",
    "    criterion: nn.CrossEntropyLoss()\n",
    "    '''\n",
    "\n",
    "    num_of_total_batches = len(dataloader)\n",
    "    num_of_total_samples = len(dataloader.dataset)\n",
    "    epoch_loss = 0\n",
    "    epoch_correct_predicted = 0\n",
    "\n",
    "    # iteriere durch alle batches im dataloader\n",
    "    for batch_index, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        # images has dimension [batch_size, colour channel, width, height]\n",
    "        # labels is a tensor with (list) with the class number\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward Propagation\n",
    "        labels_predict = model(images)\n",
    "\n",
    "        # calculate the loss based on criterion\n",
    "        loss = criterion(labels_predict, labels)\n",
    "\n",
    "        # calculate metric loss and accuracy\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        epoch_correct_predicted = epoch_correct_predicted + (labels_predict.argmax(axis=1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "    # calculate the average loss and accuracy for the epoch\n",
    "    epoch_avg_loss = epoch_loss / num_of_total_batches\n",
    "    epoch_accuracy = epoch_correct_predicted / num_of_total_samples\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Validation Loss: {epoch_avg_loss}, Validation Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "    return epoch_avg_loss, epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing implementation of train and test function\n",
    "SGD_optimizer = optim.SGD(model_cnn_1.parameters(), lr = 0.01)\n",
    "criterion_crossentropy = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(2):\n",
    "    train_modell(model_mlp_2, trainloader, criterion=criterion_crossentropy, optimizer=SGD_optimizer, verbose=True)\n",
    "    test_model(model_mlp_2, valloader, criterion=criterion_crossentropy, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainieren von Modellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Teste Modelle und Trainingsfunktionalität, indem Du nur mit einem einzigen Sample\n",
    "oder einem Batch trainierst. Damit bekommst Du zwar Overfitting, aber auch einen\n",
    "guten Test, der zeigt dass Information aus den Daten aufgenommen werden kann.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 05ibw9wv\n",
      "Sweep URL: https://wandb.ai/7ben18/del-mc1/sweeps/05ibw9wv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rc2824hu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: MLP_1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ben\\Documents\\GitHub\\fhnw-del-Deep-Learning\\del-Minichallenge-1\\wandb\\run-20230410_104639-rc2824hu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/7ben18/del-mc1/runs/rc2824hu' target=\"_blank\">frosty-sweep-1</a></strong> to <a href='https://wandb.ai/7ben18/del-mc1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/7ben18/del-mc1/sweeps/05ibw9wv' target=\"_blank\">https://wandb.ai/7ben18/del-mc1/sweeps/05ibw9wv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/7ben18/del-mc1' target=\"_blank\">https://wandb.ai/7ben18/del-mc1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/7ben18/del-mc1/sweeps/05ibw9wv' target=\"_blank\">https://wandb.ai/7ben18/del-mc1/sweeps/05ibw9wv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/7ben18/del-mc1/runs/rc2824hu' target=\"_blank\">https://wandb.ai/7ben18/del-mc1/runs/rc2824hu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid', \n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'num_epochs': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'model': {\n",
    "            'values': ['MLP_1', 'MLP_2', 'CNN_1', 'CNN_2', ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# create a sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"del-mc1\")\n",
    "\n",
    "\n",
    "# define the train function for wandb\n",
    "def wandb_train():\n",
    "    # initialize a new wandb run\n",
    "    wandb.init()\n",
    "\n",
    "    # get the hyperparameters from wandb\n",
    "    lr = wandb.config.lr\n",
    "    batch_size = wandb.config.batch_size\n",
    "    num_epochs = wandb.config.num_epochs\n",
    "    models = wandb.config.model\n",
    "    \n",
    "    if models == \"MLP_1\":\n",
    "        model = MLP_1().to(device)\n",
    "    elif models == \"MLP_2\":\n",
    "        model = MLP_2().to(device)\n",
    "    elif models == \"CNN_1\":\n",
    "        model = CNN_1().to(device)\n",
    "    elif models == \"CNN_2\":\n",
    "        model = CNN_2().to(device)\n",
    "        \n",
    "    # create model\n",
    "    #model = MLP_1().to(device)\n",
    "    \n",
    "    # get dataloader\n",
    "    trainloader, valloader = get_dataloader(batch_size=batch_size)\n",
    "\n",
    "    # create optimizer\n",
    "    SGD_optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "    # create criterion\n",
    "    criterion_crossentropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train model\n",
    "        train_loss, train_acc = train_modell(model, trainloader, criterion=criterion_crossentropy, optimizer=SGD_optimizer)\n",
    "        # test model\n",
    "        val_loss, val_acc = test_model(model, valloader, criterion=criterion_crossentropy)\n",
    "\n",
    "        # log metrics to wandb\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_acc, \"Validation Loss\": val_loss, \"Validation Accuracy\": val_acc, \"epoch\": epoch + 1})\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=wandb_train)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 4: Evaluation\n",
    "\n",
    "Bei der Evaluation ist darauf zu achten, dass das Vorgehen stets möglichst reflektiert\n",
    "erfolgt und versucht wird, die Ergebnisse zu interpretieren. Am Schluss soll auch ein\n",
    "Fazit gezogen werden, darüber welche Variante am besten funktioniert.\n",
    "\n",
    "### 1. Training mit SGD, ohne REG, ohne BN:  \n",
    "Untersuche verschiedene Modelle unterschiedlicher Komplexität, welche geeignet\n",
    "sein könnten, um das Klassifikationsproblem zu lösen. Verwende Stochastic Gradient\n",
    "Descent - ohne Beschleunigung, ohne Regularisierung (REG) und ohne Batchnorm\n",
    "(BN). Überlege Dir für jeden Fall, wie die Gewichte initialisiert werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Für jedes Modell mit gegebener Anzahl Layer und Units pro Layer führe ein\n",
    "sorgfältiges Hyper-Parameter-Tuning durch (Lernrate, Batch-Grösse). Achte\n",
    "stets darauf, dass das Training stabil läuft. Merke Dir bei jedem Training, den\n",
    "Loss, die Performance Metrik(en) inkl. Schätzfehler, die verwendete Anzahl\n",
    "Epochen, Lernrate und Batch-Grösse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'grid', \n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {\n",
    "            'values': [0.1, 0.01]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'num_epochs': {\n",
    "            'values': [20]\n",
    "        },\n",
    "        'model': {\n",
    "            'values': ['MLP_1', 'MLP_2', 'CNN_1', 'CNN_2', ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# create a sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"del-mc1\")\n",
    "\n",
    "\n",
    "# define the train function for wandb\n",
    "def wandb_train():\n",
    "    # initialize a new wandb run\n",
    "    wandb.init()\n",
    "\n",
    "    # get the hyperparameters from wandb\n",
    "    lr = wandb.config.lr\n",
    "    batch_size = wandb.config.batch_size\n",
    "    num_epochs = wandb.config.num_epochs\n",
    "    models = wandb.config.model\n",
    "    \n",
    "    if models == \"MLP_1\":\n",
    "        model = MLP_1().to(device)\n",
    "    elif models == \"MLP_2\":\n",
    "        model = MLP_2().to(device)\n",
    "    elif models == \"CNN_1\":\n",
    "        model = CNN_1().to(device)\n",
    "    elif models == \"CNN_2\":\n",
    "        model = CNN_2().to(device)\n",
    "        \n",
    "    # create model\n",
    "    #model = MLP_1().to(device)\n",
    "    \n",
    "    # get dataloader\n",
    "    trainloader, valloader = get_dataloader(batch_size=batch_size)\n",
    "\n",
    "    # create optimizer\n",
    "    SGD_optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "    # create criterion\n",
    "    criterion_crossentropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train model\n",
    "        train_loss, train_acc = train_modell(model, trainloader, criterion=criterion_crossentropy, optimizer=SGD_optimizer)\n",
    "        # test model\n",
    "        val_loss, val_acc = test_model(model, valloader, criterion=criterion_crossentropy)\n",
    "\n",
    "        # log metrics to wandb\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_acc, \"Validation Loss\": val_loss, \"Validation Accuracy\": val_acc, \"epoch\": epoch + 1})\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=wandb_train)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Variiere die Anzahl Layer und Anzahl Units pro Layer, um eine möglichst gute\n",
    "Performance zu erreichen. Falls auch CNNs (ohne Transfer-Learning)\n",
    "verwendet werden variiere auch Anzahl Filter, Kernel-Grösse, Stride, Padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Fasse die Ergebnisse zusammen in einem geeigneten Plot, bilde eine\n",
    "Synthese und folgere, welche Modell-Komplexität Dir am sinnvollsten\n",
    "erscheint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nutzen der Regularisierung\n",
    "\n",
    "Ziehe nun verschiedene Regularisierungsmethoden bei den MLP Layern in Betracht:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. L1/L2 Weight Penalty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluiere den Nutzen der Regularisierung, auch unter Berücksichtigung\n",
    "verschiedener Regularisierungsstärken.\n",
    "Beschreibe auch kurz, was allgemein das Ziel von Regularisierungsmethoden ist\n",
    "(Regularisierung im Allgemeinen, sowie auch Idee der einzelnen Methoden).\n",
    "Inwiefern wird dieses Ziel im gegebenen Fall erreicht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nutzen von Batchnorm BN (ohne REG, mit SGD)\n",
    "\n",
    "Evaluiere, ob Batchnorm etwas bringt. Beschreibe kurz, was die Idee von BN ist,\n",
    "wozu es helfen soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Nutzen von Adam (ohne BN, ohne / mit REG)\n",
    "\n",
    "Evaluiere, ob Du mit Adam bessere Resultate erzielen kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 5: Präsentation, Bericht\n",
    "\n",
    "1. Präsentation (~10m): Kurze Präsentation mit Diskussion der wichtigsten Ergebnisse.\n",
    "Q&A (~10min): Klärung von Verständnisfragen zu Stochastic Gradient Descent,\n",
    "Parameter Tuning, Regularisierung, Batchnorm und Optimizers.\n",
    "\n",
    "2. Bericht in Form eines (!) gut dokumentierten, übersichtlichen Jupyter Notebooks.\n",
    "Dieses soll schliesslich auch abgegeben werden und dem Fachexperten erlauben, die\n",
    "Schritte nachzuvollziehen (allenfalls auch das Training erneut laufen zu lassen)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "del",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57a2129798a0136cc4b39e35616bc148f9b4640b819a023c84b838bc0f8f158d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
