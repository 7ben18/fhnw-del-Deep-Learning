{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    del Mini-Challenge 2 <br> \n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Si Ben Tran im HS 2023.<br>Bachelor of Science FHNW in Data Science.\n",
    "</p>\n",
    "\n",
    "\n",
    "Ziel:  \n",
    "Vertiefung in ein eher aktuelles Paper aus der Forschung und Umsetzung eines darin beschriebenen oder verwandten Tasks - gemäss Vereinbarung mit dem Fachcoach. \n",
    "\n",
    "Beispiel:  \n",
    "Implementiere, trainiere und validiere ein Deep Learning Modell für Image Captioning wie beschrieben im Paper Show and Tell.\n",
    "\n",
    "Zeitlicher Rahmen:  \n",
    "Wird beim Schritt 1 verbindlich festgelegt.\n",
    "\n",
    "Beurteilung:  \n",
    "Beurteilt wird auf Basis des abgegebenen Notebooks:  \n",
    "•\tVollständige und korrekte Umsetzung der vereinbarten Aufgabestellung.  \n",
    "•\tKlare, gut-strukturierte Umsetzung.   \n",
    "•\tSchlüssige Beschreibung und Interpretation der Ergebnisse. Gut gewählte und gut kommentierten Plots und Tabellen.  \n",
    "•\tVernünftiger Umgang mit (Computing-)Ressourcen.  \n",
    "•\tVerständliche Präsentation der Ergebnisse.  \n",
    "\n",
    "Referenzen, Key Words  \n",
    "•\tWord Embedding (z.B. word2vec, glove), um Wörter in numerische Vektoren in einem geeignet dimensionierten Raum zu mappen. Siehe z.B. Andrew Ng, Coursera: [Link](https://www.coursera.org/lecture/nlp-sequence-models/learning-word-embeddings-APM5s)      \n",
    "•\tBild Embedding mittels vortrainierten (evt. retrained) Netzwerken wie beispielsweise ResNet, GoogLeNet, EfficientNet oder ähnlich Transfer-Learning.  \n",
    "•\tSeq2Seq Models bekannt für Sprach-Übersetzung. \n",
    "\n",
    "Daten:   \n",
    "•\tGemäss Vereinbarung (für Captioning: [Flickr8k-Daten](https://www.kaggle.com/adityajn105/flickr8k/activity)).\n",
    "\n",
    "•\tAbsprache/Beschluss mit Coach und Beschluss, was evaluiert werden soll.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.gpu_setup import DeviceSetup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_setup = DeviceSetup(seed=42)\n",
    "device_setup.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Daten\n",
    "Wir erkennen bei der Spalte *image*, das ein jpg. Bilddatei mehrere *catpion* hat.\n",
    "\n",
    "Bei der Visualisierung der Bilder erkenne wir:\n",
    "- Personen oder Tiere\n",
    "- Unterschiedliche Grössen\n",
    "- Unterschiedliche Auflösung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    def __init__(self, image_path, captions_path):\n",
    "        self.image_path = image_path\n",
    "        self.data = pd.read_csv(captions_path)\n",
    "\n",
    "\n",
    "    def _get_image_unique(self):\n",
    "        \"\"\"\n",
    "        This method returns a list of unique image IDs.\n",
    "        \"\"\"\n",
    "        image_unique = self.data['image'].unique()\n",
    "        return image_unique\n",
    "    \n",
    "    def _get_word_counts(self):\n",
    "        \"\"\"\n",
    "        This method returns a list of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self.data['caption'].apply(str.split).apply(len)\n",
    "        return word_counts\n",
    "    \n",
    "    def _read_image(self, image_id):\n",
    "        \"\"\"\n",
    "        This method reads an image from a specific path and returns the image object.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.image_path + \"/\" + image_id)\n",
    "        return image\n",
    "\n",
    "    def _get_captions(self, image_id):\n",
    "        \"\"\"\n",
    "        This method retrieves the captions associated with an image ID from the data dictionary.\n",
    "        \"\"\"\n",
    "        captions = []\n",
    "        for i in range(len(self.data)):\n",
    "            if self.data['image'][i] == image_id:\n",
    "                captions.append(self.data['caption'][i])\n",
    "        captions = '\\n'.join(captions)\n",
    "        return captions\n",
    "\n",
    "    def plot_n_m_image_caption(self, n, m):\n",
    "        \"\"\"\n",
    "        This method plots a grid of n x m images along with their captions.\n",
    "        \"\"\"\n",
    "        image_unique = self._get_image_unique()\n",
    "        fig, ax = plt.subplots(n, m, figsize=(16, 20))\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                index = np.random.randint(0, len(image_unique))\n",
    "                image_id = image_unique[index]\n",
    "                image = self._read_image(image_id)\n",
    "                captions = self._get_captions(image_id)\n",
    "                ax[i, j].imshow(np.asarray(image))\n",
    "                ax[i, j].set_title(captions)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_image_size(self):\n",
    "        \"\"\"\n",
    "        This method plots a grid of n x m images along with their captions.\n",
    "        \"\"\"\n",
    "        image_unique = self._get_image_unique()\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        # set range of x and y axis\n",
    "        ax.set_xlabel('width of image')\n",
    "        ax.set_ylabel('height of image')\n",
    "        for i in range(len(image_unique)):\n",
    "            image_id = image_unique[i]\n",
    "            image = self._read_image(image_id)\n",
    "            width, height = image.size\n",
    "            ax.scatter(width, height)\n",
    "        ax.set_title('Distribution size of images')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # plot caption distribution word length\n",
    "    def plot_caption_distribution(self):\n",
    "        \"\"\"\n",
    "        This method plots the distribution of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.hist(word_counts, bins=25, color = 'limegreen', edgecolor='black', linewidth=1.2)\n",
    "        plt.title(\"Distribution of Number of Words per Caption\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "    # get statistical summary of caption distribution word length\n",
    "    def get_caption_distribution(self):\n",
    "        \"\"\"\n",
    "        This method prints the statistical summary of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        print(word_counts.describe(percentiles=[0.25, 0.5, 0.75, 0.95]))\n",
    "\n",
    "    def plot_caption_ecdf(self):\n",
    "        \"\"\"\n",
    "        This method plots the ECDF of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        word_counts_sorted = word_counts.sort_values()\n",
    "        y = np.arange(1, len(word_counts_sorted) + 1) / len(word_counts_sorted)  \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(word_counts_sorted, y, color='limegreen')\n",
    "        plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "        plt.axvline(x=19, color='r', linestyle='-')\n",
    "        plt.xticks(np.arange(np.min(word_counts_sorted), np.max(word_counts_sorted), 1.0))\n",
    "        plt.title(\"ECDF of Number of Words per Caption\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Proportion\")\n",
    "        plt.show()\n",
    "\n",
    "    # plot most commen words\n",
    "    def plot_most_common_words(self):\n",
    "        \"\"\"\n",
    "        This method plots the most common words in the captions.\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        word_counts = self.data['caption'].apply(str.split).apply(Counter).sum()\n",
    "        word_counts = pd.DataFrame.from_dict(word_counts, orient='index').reset_index()\n",
    "        word_counts.columns = ['word', 'count']\n",
    "        word_counts = word_counts.sort_values(by='count', ascending=False)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(word_counts['word'][:20], word_counts['count'][:20], color = 'limegreen', edgecolor='black', linewidth=1.2)\n",
    "        plt.title(\"Most Common Words in Captions\")\n",
    "        plt.xlabel(\"Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/Flickr8K/images/\"\n",
    "captions_path = \"data/Flickr8K/captions.txt\"\n",
    "\n",
    "flicker_data_explorer = DataExplorer(image_path, captions_path)\n",
    "flicker_data = flicker_data_explorer.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flicker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualisierungen der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_n_m_image_caption(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Grössen der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_image_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Caption Länge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_caption_distribution()\n",
    "#flicker_data_explorer.plot_caption_ecdf()\n",
    "#flicker_data_explorer.get_caption_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Häufigste Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_most_common_words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Preprocessing der Bilder\n",
    "\n",
    "Wir werden Die Bilder wie folgt vorbereiten, damit das Model die Bilder verarbeiten kann: \n",
    "\n",
    "`ToPILImage()`: Dieser Schritt konvertiert das Eingabebild in ein PIL (Python Imaging Library) Bildformat. Dies ist erforderlich, wenn das Eingabebild nicht bereits im PIL-Format vorliegt.\n",
    "\n",
    "`CenterCrop((500, 500))`: Hier wird das Bild auf eine Größe von 500x500 Pixel zentriert zugeschnitten. Dies ist nützlich, um das Bild auf eine bestimmte Größe zu bringen und sicherzustellen, dass wichtige Merkmale in der Mitte erhalten bleiben.\n",
    "\n",
    "`Resize((224, 224))`: Das Bild wird auf eine Größe von 224x224 Pixel skaliert. Dies ist eine häufig verwendete Größe für viele neuronale Netzwerke, insbesondere in der Bildklassifikation, wie z.B. Convolutional Neural Networks (CNNs).\n",
    "\n",
    "`ToTensor()`: Hier wird das Bild in einen PyTorch-Tensor konvertiert. Die meisten neuronalen Netzwerke in PyTorch und anderen Frameworks arbeiten mit Tensoren als Eingabe.\n",
    "\n",
    "`Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Diese Transformation normalisiert die Pixelwerte des Bildes. Dies ist wichtig, um sicherzustellen, dass die Werte im Eingangsbild in einem bestimmten Bereich liegen. Die angegebenen Mittelwerte und Standardabweichungen sind typische Werte für die Normalisierung von Bildern, die auf dem ImageNet-Datensatz trainiert wurden.\n",
    "\n",
    "`RandomHorizontalFlip`: Führt mit einer Wahrscheinlichkeit von `horizontal_flip_prob` eine zufällige horizontale Spiegelung des Bildes durch.\n",
    "\n",
    "`RandomVerticalFlip`: Führt mit einer Wahrscheinlichkeit von `vertical_flip_prob` eine zufällige vertikale Spiegelung des Bildes durch.\n",
    "\n",
    "`RandomRotation`: Führt eine zufällige Rotation des Bildes um den angegebenen Winkel (`rotation_degree` Grad) durch.\n",
    "\n",
    "`ColorJitter`: Verändert die Helligkeit, den Kontrast, die Sättigung und den Farbton des Bildes zufällig, um die Farbvariationen zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor, ToPILImage, Normalize\n",
    "\n",
    "target_size = (224, 224)\n",
    "center_crpp = (500, 500)\n",
    "mean_values = [0.485, 0.456, 0.406]\n",
    "std_values = [0.229,0.224,0.225]\n",
    "\n",
    "# Transformations for the image\n",
    "image_transformations = Compose([\n",
    "    CenterCrop(center_crpp),\n",
    "    Resize(target_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_values,\n",
    "                std=std_values)\n",
    "])\n",
    "\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, RandomVerticalFlip, ColorJitter\n",
    "\n",
    "rotation_degree = 45\n",
    "horizontal_flip_prob = 0.5\n",
    "vertical_flip_prob = 0.5\n",
    "\n",
    "image_transforms_augmented = Compose([\n",
    "    ToPILImage(),\n",
    "    RandomHorizontalFlip(p=horizontal_flip_prob),\n",
    "    RandomVerticalFlip(p=vertical_flip_prob),\n",
    "    RandomRotation(degrees=rotation_degree),\n",
    "    ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    CenterCrop(center_crpp),\n",
    "    Resize(target_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_values,\n",
    "                std=std_values)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4 PreProcessing von Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class CaptionProcessor:\n",
    "    def __init__(self, max_length: int):\n",
    "        self.max_length = max_length\n",
    "        self.vocabulary = []\n",
    "        self.start_token = \"<sos>\"\n",
    "        self.stop_token = \"<eos>\"\n",
    "        self.unknown_token = \"<unk>\"\n",
    "        self.padding_token = \"<pad>\"\n",
    "        self.token_to_index = {}  \n",
    "        self.index_to_token = {}\n",
    "\n",
    "    def create_vocab(self, dataframe, caption_column: str):\n",
    "        \"\"\"\n",
    "        Create a vocabulary from a DataFrame containing captions.\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): A DataFrame containing caption data.\n",
    "            caption_column (str): The name of the column in the DataFrame that contains captions.\n",
    "        \"\"\"\n",
    "        # Join all the captions in the specified column of the DataFrame into a single string\n",
    "        all_captions = \" \".join(dataframe[caption_column].values)\n",
    "        # make all captions lowercase \n",
    "        all_captions = all_captions.lower()\n",
    "        # create a set of all unique words in the joined captions string\n",
    "        all_captions = set(all_captions.split())\n",
    "        # convert the set of unique words to a list  \n",
    "        all_captions = list(all_captions)\n",
    "        # join all the unique words into a single string separated by spaces\n",
    "        all_captions = \" \".join(all_captions) \n",
    "        # Tokenize the joined captions into words and convert them to lowercase\n",
    "        self.vocabulary = nltk.tokenize.word_tokenize(all_captions)\n",
    "        # Add special tokens to the vocabulary\n",
    "        self.vocabulary = [self.start_token, self.stop_token, self.unknown_token, self.padding_token] + self.vocabulary\n",
    "        self.token_to_index = {token: index for index, token in enumerate(self.vocabulary)}\n",
    "        self.index_to_token = {index: token for index, token in enumerate(self.vocabulary)}\n",
    "\n",
    "\n",
    "    def caption_to_tokens(self, dataframe, caption_column):\n",
    "        \"\"\"\n",
    "        Preprocess captions in a DataFrame by tokenizing and adding start and stop tokens.\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): A DataFrame containing caption data.\n",
    "            caption_column (str): The name of the column in the DataFrame that contains captions.\n",
    "        Returns:\n",
    "            pandas.DataFrame: The input DataFrame with an additional \"tokenized_caption\" column \n",
    "                              containing tokenized captions with start and stop tokens added.\n",
    "        \"\"\"\n",
    "        # Convert captions to lowercase\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[caption_column].apply(lambda x: x.lower())\n",
    "        # Split caption into tokens\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[caption_column].apply(lambda x: x.split())\n",
    "        # Add start token at the beginning\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[\"tokenized_caption\"].apply(lambda x: [self.start_token] + x)\n",
    "        # Add stop token at the end\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[\"tokenized_caption\"].apply(lambda x: x + [self.stop_token])\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    def tokens_to_indices(self, tokens):\n",
    "        \"\"\"\n",
    "        Converts a list of tokens into their corresponding indices in the vocabulary.\n",
    "        Args:\n",
    "            tokens (list): List of tokens representing a caption.\n",
    "        Returns:\n",
    "            list: List of indices representing the caption.\n",
    "        \"\"\"\n",
    "        indices = [self.token_to_index[token] if token in self.vocabulary else self.token_to_index[self.unknown_token] for token in tokens]\n",
    "        # add padding to max length\n",
    "        indices_padding = indices + [self.token_to_index[self.padding_token]] * (self.max_length - len(indices))\n",
    "        # truncate to max length\n",
    "        indices_padding = indices_padding[:self.max_length]\n",
    "        return indices_padding\n",
    "    \n",
    "    def indices_to_tokens(self, indices):\n",
    "        \"\"\"\n",
    "        Converts a list of indices to their corresponding tokens.\n",
    "        Args:\n",
    "            indices (list): List of indices representing a caption.\n",
    "        Returns:\n",
    "            list: List of tokens corresponding to the input indices.\n",
    "        \"\"\"\n",
    "        tokens = [self.index_to_token[index] if index in self.index_to_token else self.unknown_token for index in indices]\n",
    "        return tokens\n",
    "\n",
    "    def tokens_to_caption(self, tokens):\n",
    "        \"\"\"\n",
    "        Converts a list of tokens into a human-readable caption, handling unknown words, padding, and maximum length.\n",
    "        Args:\n",
    "            tokens (list): List of tokens representing a caption.\n",
    "        Returns:\n",
    "            str: Human-readable caption.\n",
    "        \"\"\"\n",
    "        # Lower-case every token in the list\n",
    "        tokens = [token for token in tokens]\n",
    "        # Remove start and stop tokens\n",
    "        tokens = [token for token in tokens if token not in [self.start_token, self.stop_token, self.unknown_token, self.padding_token]]\n",
    "        # Replace unknown tokens with \"<unk>\"\n",
    "        #tokens = [token if token in self.vocabulary else self.unknown_token for token in tokens]\n",
    "        # Exclude padding tokens\n",
    "        #tokens = [token for token in tokens if token != self.padding_token]\n",
    "        # Truncate the caption to the maximum length\n",
    "        tokens = tokens[:self.max_length]\n",
    "        # Join the tokens to create the caption\n",
    "        caption = \" \".join(tokens)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_processor = CaptionProcessor(max_length=20)\n",
    "caption_processor.create_vocab(flicker_data, \"caption\")\n",
    "flicker_data_tokenized = caption_processor.caption_to_tokens(flicker_data, \"caption\")\n",
    "display(flicker_data_tokenized.head())\n",
    "print(\"Vocab size:\", len(caption_processor.vocabulary))\n",
    "\n",
    "# testing tokens_to_indices\n",
    "test_tokens = [\"<sos>\", \"a\", \"group\", \"of\", \"people\", \"are\", \"Ben\", \"standing\", \"around\", \"a\", \"table\", \"Ben\", \"<pad>\", \"<pad>\", \"<pad>\", \"<eos>\"]\n",
    "test_indices = caption_processor.tokens_to_indices(test_tokens)\n",
    "print(test_indices)\n",
    "\n",
    "# testing indices_to_tokens\n",
    "test_indices = [0, 476674, 476492, 476493, 476512, 476605, 2, 476596, 476476, 476674, 476103, 2, 3, 3, 3, 1, 3, 3, 3, 3]\n",
    "test_tokens = caption_processor.indices_to_tokens(test_indices)\n",
    "print(test_tokens)\n",
    "\n",
    "# testing tokens_to_caption\n",
    "test_tokens = ['<sos>', 'a', 'group', 'of', 'people', 'are', 'standing', 'around', 'a', 'table', '<unk>', '<eos>']\n",
    "test_caption = caption_processor.tokens_to_caption(test_tokens)\n",
    "print(test_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5 Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the tokenized_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# get all unique images\n",
    "images = flicker_data_tokenized[\"image\"].unique()\n",
    "\n",
    "# Split the images into train, validation, and test sets Train/Val/Test split (60/20/20)\n",
    "train_images, val_test_images = train_test_split(images, test_size=0.4, random_state=42)\n",
    "val_images, test_images = train_test_split(val_test_images, test_size=0.5, random_state=42)\n",
    "\n",
    "# Then, filter the data based on the split sets\n",
    "train_df = flicker_data_tokenized[flicker_data_tokenized[\"image\"].isin(train_images)]\n",
    "val_df = flicker_data_tokenized[flicker_data_tokenized[\"image\"].isin(val_images)]\n",
    "test_df = flicker_data_tokenized[flicker_data_tokenized[\"image\"].isin(test_images)]\n",
    "\n",
    "# display the data\n",
    "display(\"train_data\", train_df)\n",
    "display(\"val_data\", val_df)\n",
    "display(\"test_data\", test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from multiprocessing import cpu_count\n",
    "from src.flickerdataset import Flicker8kDataset\n",
    "\n",
    "# Create the datasets\n",
    "train_data = Flicker8kDataset(train_df, image_path, image_transformations, caption_processor)\n",
    "val_data = Flicker8kDataset(val_df, image_path, image_transformations, caption_processor)\n",
    "test_data = Flicker8kDataset(test_df, image_path, image_transformations, caption_processor)\n",
    "\n",
    "# get dimension of the data, and the first data\n",
    "print(\"train_data\", len(train_data))\n",
    "print(\"val_data\", len(val_data))\n",
    "print(\"test_data\", len(test_data))\n",
    "image_train, caption_train = train_data[1]\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=False, num_workers=0, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, pin_memory=False, num_workers=0, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, pin_memory=False, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in val_loader: {len(val_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dimension of the data, and the first data\n",
    "image_train, caption_train = next(iter(val_loader))\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 7 CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Resnet18 model in this class \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # define embedding dimension\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # Use ResNet-18 with pre-trained weights\n",
    "\n",
    "        # freeze all layers except the last linear layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "\n",
    "        # overwrite the last layer\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, self.embedding_dim), \n",
    "                                       nn.BatchNorm1d(self.embedding_dim, momentum=0.01))\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "encoder = EncoderCNN(embedding_dim)\n",
    "encoder.to(device_setup.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the encoder\n",
    "image_train, caption_train = next(iter(train_loader))\n",
    "image_train = image_train.to(device_setup.device)\n",
    "caption_train = caption_train.to(device_setup.device)\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)\n",
    "features = encoder(image_train)\n",
    "print(\"features\", features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8 LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DecoderLSTM model \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        # define the properties\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        # define the layers\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Decode feature vectors and generates captions.\n",
    "        Args:\n",
    "            features (torch.Tensor): Tensor of extracted feature vectors from images.\n",
    "            captions (torch.Tensor): Tensor of captions.\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of predicted captions.\n",
    "        \"\"\"\n",
    "        # Remove end token from captions\n",
    "        captions = captions[:, :-1]\n",
    "        # Embed the captions\n",
    "        embeddings = self.embed(captions)\n",
    "        # Concatenate the feature vectors and embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        # Pass the embeddings through the LSTM cells\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        # Reshape outputs to be (batch_size * sequence_length, hidden_size)\n",
    "        outputs = hiddens.reshape(-1, hiddens.size(2))\n",
    "        # Pass the outputs through the linear layer\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def greedy_sample(self, features, states=None, max_length=20):\n",
    "        batch_size = features.size(0)  # Get the batch size\n",
    "        sampled_ids = [[] for _ in range(batch_size)]  # List to store the sampled captions for each image in the batch\n",
    "        \n",
    "        # Prepare the initial input for LSTM, which is the features tensor\n",
    "        inputs = features.unsqueeze(1)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            hiddens, states = self.lstm(inputs, states)  # Pass the input through LSTM\n",
    "            outputs = self.linear(hiddens.squeeze(1))  # Pass the LSTM outputs through the linear layer\n",
    "\n",
    "            predicted = outputs.argmax(1)  # Get the predicted word indices\n",
    "            for j in range(batch_size):\n",
    "                # break of eos token \n",
    "                if predicted[j].item() == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    sampled_ids[j].append(predicted[j].item())  # Append the predicted word index to the respective caption list\n",
    "\n",
    "            inputs = self.embed(predicted)  # Prepare the input for the next time step\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        return sampled_ids\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the linear layer.\n",
    "        \"\"\"\n",
    "        self.linear.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.linear.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(caption_processor.vocabulary)\n",
    "num_layers = 1\n",
    "decoder = DecoderLSTM(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "decoder.to(device_setup.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the decoder\n",
    "features = features.to(device_setup.device)\n",
    "captions = caption_train.to(device_setup.device)\n",
    "outputs = decoder(features, captions)\n",
    "print(\"outputs\", outputs.shape)\n",
    "print(\"outputs\", outputs)\n",
    "# test the decoder sample\n",
    "sampled_ids = decoder.greedy_sample(features)\n",
    "print(\"sampled_ids\", sampled_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        # define the properties\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def greedy_sample(self, images):\n",
    "        features = self.encoder(images)\n",
    "        sampled_ids = self.decoder.greedy_sample(features)\n",
    "        return sampled_ids\n",
    "    \n",
    "    # train model\n",
    "    def train_step(self, images, captions, criterion, optimizer):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = self.forward(images, captions)\n",
    "        loss = criterion(outputs, captions.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    # validation model\n",
    "    def val_step(self, images, captions, criterion):\n",
    "        # forward\n",
    "        outputs = self.forward(images, captions)\n",
    "        loss = criterion(outputs, captions.reshape(-1))\n",
    "        return loss.item()\n",
    "    \n",
    "    # test model\n",
    "    def test_step(self, images):\n",
    "        sampled_ids = self.greedy_sample(images)\n",
    "        return sampled_ids\n",
    "    \n",
    "    # save model\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    # load model\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "# define the properties\n",
    "hidden_size = 512\n",
    "vocab_size = len(caption_processor.vocabulary)\n",
    "num_layers = 1\n",
    "# define the model\n",
    "decoder = DecoderLSTM(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "decoder.to(device_setup.device)\n",
    "# define the model\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "model.to(device_setup.device)\n",
    "print(model)\n",
    "\n",
    "# test the model\n",
    "image_train, caption_train = next(iter(train_loader))\n",
    "image_train = image_train.to(device_setup.device)\n",
    "caption_train = caption_train.to(device_setup.device)\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)\n",
    "outputs = model(image_train, caption_train)\n",
    "print(\"outputs\", outputs.shape)\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=caption_processor.token_to_index[caption_processor.padding_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# define the number of epochs\n",
    "\n",
    "num_epochs = 1\n",
    "# define the path to save the model\n",
    "model_path = \"models/encoder_decoder.pt\"\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # train the model\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (images, captions) in enumerate(train_loader):\n",
    "        # move images and captions to gpu if available\n",
    "        images = images.to(device_setup.device)\n",
    "        captions = captions.to(device_setup.device)\n",
    "        # train step\n",
    "        loss = model.train_step(images, captions, criterion, optimizer)\n",
    "        train_loss += loss\n",
    "        # print statistics\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss}\")\n",
    "    # validation the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for i, (images, captions) in enumerate(val_loader):\n",
    "        # move images and captions to gpu if available\n",
    "        images = images.to(device_setup.device)\n",
    "        captions = captions.to(device_setup.device)\n",
    "        # validation step\n",
    "        loss = model.val_step(images, captions, criterion)\n",
    "        val_loss += loss\n",
    "        # print statistics\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss}\")\n",
    "    # print statistics\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "    # save the model\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9 Encoder-Decoder Architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        for images, captions in tqdm(train_loader, desc=\"Training\"):\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self(images, captions)\n",
    "            targets = pack_padded_sequence(captions, [len(caption) for caption in captions], batch_first=True, enforce_sorted=False)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, captions in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images, captions = images.to(device), captions.to(device)\n",
    "                outputs = self(images, captions)\n",
    "\n",
    "                targets = pack_padded_sequence(captions, [len(caption) for caption in captions], batch_first=True, enforce_sorted=False)[0]\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "encoder_decoder = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "# Move the model to a device\n",
    "device = device_setup.device\n",
    "encoder_decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(encoder_decoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = encoder_decoder.train_one_epoch(train_loader, criterion, optimizer, device)\n",
    "    val_loss = encoder_decoder.validate(val_loader, criterion, device)\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10 Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder_decoder model\n",
    "torch.save(encoder_decoder.state_dict(), \"models/encoder_decoder_model_5_epoch_train_val_set.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder.load_state_dict(torch.load(\"models/encoder_decoder_model_5_epoch_train_val_set.pth\"))\n",
    "encoder_decoder.to(device_setup.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 11 Evalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def visualize_results(data_loader, encoder_decoder, caption_processor, device, image_path, num_images=8):\n",
    "    encoder_decoder.eval()\n",
    "\n",
    "    # Get a batch of data\n",
    "    for images, _ in iter(data_loader):\n",
    "        break\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Generate captions\n",
    "    predicted_captions = []\n",
    "    for i in range(0, num_images * 5, 5):  # Iterating over every 5th image\n",
    "        if i >= len(images):\n",
    "            break\n",
    "        image = images[i]\n",
    "        features = encoder_decoder.encoder(image.unsqueeze(0))\n",
    "        sampled_ids = encoder_decoder.decoder.greedy_sample(features)\n",
    "        tokens = [caption_processor.indices_to_tokens(ids) for ids in sampled_ids]\n",
    "        captions = [caption_processor.tokens_to_caption(token_list) for token_list in tokens]\n",
    "        predicted_captions.append(captions)\n",
    "\n",
    "    # Ensure we have enough rows in the subplot\n",
    "    num_rows = min(num_images, len(predicted_captions))\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, 1, figsize=(10, 20))\n",
    "    if num_rows == 1:\n",
    "        axs = [axs]  # Make axs iterable even if it's a single Axes object\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        # Adjust the index to account for the 5-image groups\n",
    "        adjusted_index = i * 5\n",
    "        image_file = os.path.join(image_path, data_loader.dataset.dataframe.iloc[adjusted_index][\"image\"])\n",
    "        if not os.path.exists(image_file):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(image_file)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "        true_captions = data_loader.dataset.dataframe[data_loader.dataset.dataframe[\"image\"] == data_loader.dataset.dataframe.iloc[adjusted_index][\"image\"]][\"caption\"].tolist()\n",
    "        true_captions_text = \"\\n\".join(true_captions)\n",
    "\n",
    "        ax.set_title(f'Predicted: {predicted_captions[i][0]}\\nTrue: {true_captions_text}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_results(test_loader, encoder_decoder, caption_processor, device, image_path, num_images=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
