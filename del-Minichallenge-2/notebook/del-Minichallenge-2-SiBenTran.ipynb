{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    del Mini-Challenge 2 <br> \n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Si Ben Tran im HS 2023.<br>Bachelor of Science FHNW in Data Science.\n",
    "</p>\n",
    "\n",
    "\n",
    "Ziel:  \n",
    "Vertiefung in ein eher aktuelles Paper aus der Forschung und Umsetzung eines darin beschriebenen oder verwandten Tasks - gemäss Vereinbarung mit dem Fachcoach. \n",
    "\n",
    "Beispiel:  \n",
    "Implementiere, trainiere und validiere ein Deep Learning Modell für Image Captioning wie beschrieben im Paper Show and Tell.\n",
    "\n",
    "Zeitlicher Rahmen:  \n",
    "Wird beim Schritt 1 verbindlich festgelegt.\n",
    "\n",
    "Beurteilung:  \n",
    "Beurteilt wird auf Basis des abgegebenen Notebooks:  \n",
    "•\tVollständige und korrekte Umsetzung der vereinbarten Aufgabestellung.  \n",
    "•\tKlare, gut-strukturierte Umsetzung.   \n",
    "•\tSchlüssige Beschreibung und Interpretation der Ergebnisse. Gut gewählte und gut kommentierten Plots und Tabellen.  \n",
    "•\tVernünftiger Umgang mit (Computing-)Ressourcen.  \n",
    "•\tVerständliche Präsentation der Ergebnisse.  \n",
    "\n",
    "Referenzen, Key Words  \n",
    "•\tWord Embedding (z.B. word2vec, glove), um Wörter in numerische Vektoren in einem geeignet dimensionierten Raum zu mappen. Siehe z.B. Andrew Ng, Coursera: [Link](https://www.coursera.org/lecture/nlp-sequence-models/learning-word-embeddings-APM5s)      \n",
    "•\tBild Embedding mittels vortrainierten (evt. retrained) Netzwerken wie beispielsweise ResNet, GoogLeNet, EfficientNet oder ähnlich Transfer-Learning.  \n",
    "•\tSeq2Seq Models bekannt für Sprach-Übersetzung. \n",
    "\n",
    "Daten:   \n",
    "•\tGemäss Vereinbarung (für Captioning: [Flickr8k-Daten](https://www.kaggle.com/adityajn105/flickr8k/activity)).\n",
    "\n",
    "•\tAbsprache/Beschluss mit Coach und Beschluss, was evaluiert werden soll.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# path setup\n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "# Data Science Libraries\n",
    "import tqdm \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor, ToPILImage, Normalize\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, RandomVerticalFlip, ColorJitter\n",
    "\n",
    "# Custom Modules\n",
    "from src.gpu_setup import DeviceSetup\n",
    "from src.flickerdataset import Flicker8kDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 980\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device_setup = DeviceSetup(seed=42)\n",
    "device_setup.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Daten\n",
    "Wir erkennen bei der Spalte *image*, das ein jpg. Bilddatei mehrere *catpion* hat.\n",
    "\n",
    "Bei der Visualisierung der Bilder erkenne wir:\n",
    "- Personen oder Tiere\n",
    "- Unterschiedliche Grössen\n",
    "- Unterschiedliche Auflösung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    def __init__(self, image_path, captions_path):\n",
    "        self.image_path = image_path\n",
    "        self.data = pd.read_csv(captions_path)\n",
    "\n",
    "\n",
    "    def _get_image_unique(self):\n",
    "        \"\"\"\n",
    "        This method returns a list of unique image IDs.\n",
    "        \"\"\"\n",
    "        image_unique = self.data['image'].unique()\n",
    "        return image_unique\n",
    "    \n",
    "    def _get_word_counts(self):\n",
    "        \"\"\"\n",
    "        This method returns a list of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self.data['caption'].apply(str.split).apply(len)\n",
    "        return word_counts\n",
    "    \n",
    "    def _read_image(self, image_id):\n",
    "        \"\"\"\n",
    "        This method reads an image from a specific path and returns the image object.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.image_path + \"/\" + image_id)\n",
    "        return image\n",
    "\n",
    "    def _get_captions(self, image_id):\n",
    "        \"\"\"\n",
    "        This method retrieves the captions associated with an image ID from the data dictionary.\n",
    "        \"\"\"\n",
    "        captions = []\n",
    "        for i in range(len(self.data)):\n",
    "            if self.data['image'][i] == image_id:\n",
    "                captions.append(self.data['caption'][i])\n",
    "        captions = '\\n'.join(captions)\n",
    "        return captions\n",
    "\n",
    "    def plot_n_m_image_caption(self, n, m):\n",
    "        \"\"\"\n",
    "        This method plots a grid of n x m images along with their captions.\n",
    "        \"\"\"\n",
    "        image_unique = self._get_image_unique()\n",
    "        fig, ax = plt.subplots(n, m, figsize=(16, 20))\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                index = np.random.randint(0, len(image_unique))\n",
    "                image_id = image_unique[index]\n",
    "                image = self._read_image(image_id)\n",
    "                captions = self._get_captions(image_id)\n",
    "                ax[i, j].imshow(np.asarray(image))\n",
    "                ax[i, j].set_title(captions)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_image_size(self):\n",
    "        \"\"\"\n",
    "        This method plots a grid of n x m images along with their captions.\n",
    "        \"\"\"\n",
    "        image_unique = self._get_image_unique()\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        # set range of x and y axis\n",
    "        ax.set_xlabel('width of image')\n",
    "        ax.set_ylabel('height of image')\n",
    "        for i in range(len(image_unique)):\n",
    "            image_id = image_unique[i]\n",
    "            image = self._read_image(image_id)\n",
    "            width, height = image.size\n",
    "            ax.scatter(width, height)\n",
    "        ax.set_title('Distribution size of images')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # plot caption distribution word length\n",
    "    def plot_caption_distribution(self):\n",
    "        \"\"\"\n",
    "        This method plots the distribution of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.hist(word_counts, bins=25, color = 'limegreen', edgecolor='black', linewidth=1.2)\n",
    "        plt.title(\"Distribution of Number of Words per Caption\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "    # get statistical summary of caption distribution word length\n",
    "    def get_caption_distribution(self):\n",
    "        \"\"\"\n",
    "        This method prints the statistical summary of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        print(word_counts.describe(percentiles=[0.25, 0.5, 0.75, 0.95]))\n",
    "\n",
    "    def plot_caption_ecdf(self):\n",
    "        \"\"\"\n",
    "        This method plots the ECDF of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        word_counts_sorted = word_counts.sort_values()\n",
    "        y = np.arange(1, len(word_counts_sorted) + 1) / len(word_counts_sorted)  \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(word_counts_sorted, y, color='limegreen')\n",
    "        plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "        plt.axvline(x=19, color='r', linestyle='-')\n",
    "        plt.xticks(np.arange(np.min(word_counts_sorted), np.max(word_counts_sorted), 1.0))\n",
    "        plt.title(\"ECDF of Number of Words per Caption\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Proportion\")\n",
    "        plt.show()\n",
    "\n",
    "    # plot most commen words\n",
    "    def plot_most_common_words(self):\n",
    "        \"\"\"\n",
    "        This method plots the most common words in the captions.\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        word_counts = self.data['caption'].apply(str.split).apply(Counter).sum()\n",
    "        word_counts = pd.DataFrame.from_dict(word_counts, orient='index').reset_index()\n",
    "        word_counts.columns = ['word', 'count']\n",
    "        word_counts = word_counts.sort_values(by='count', ascending=False)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(word_counts['word'][:20], word_counts['count'][:20], color = 'limegreen', edgecolor='black', linewidth=1.2)\n",
    "        plt.title(\"Most Common Words in Captions\")\n",
    "        plt.xlabel(\"Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/Flickr8K/images/\"\n",
    "captions_path = \"data/Flickr8K/captions.txt\"\n",
    "\n",
    "flicker_data_explorer = DataExplorer(image_path, captions_path)\n",
    "flicker_data = flicker_data_explorer.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flicker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualisierungen der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_n_m_image_caption(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Grössen der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_image_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Caption Länge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_caption_distribution()\n",
    "#flicker_data_explorer.plot_caption_ecdf()\n",
    "#flicker_data_explorer.get_caption_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Häufigste Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_most_common_words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Preprocessing der Bilder\n",
    "\n",
    "Wir werden Die Bilder wie folgt vorbereiten, damit das Model die Bilder verarbeiten kann: \n",
    "\n",
    "`ToPILImage()`: Dieser Schritt konvertiert das Eingabebild in ein PIL (Python Imaging Library) Bildformat. Dies ist erforderlich, wenn das Eingabebild nicht bereits im PIL-Format vorliegt.\n",
    "\n",
    "`CenterCrop((500, 500))`: Hier wird das Bild auf eine Größe von 500x500 Pixel zentriert zugeschnitten. Dies ist nützlich, um das Bild auf eine bestimmte Größe zu bringen und sicherzustellen, dass wichtige Merkmale in der Mitte erhalten bleiben.\n",
    "\n",
    "`Resize((224, 224))`: Das Bild wird auf eine Größe von 224x224 Pixel skaliert. Dies ist eine häufig verwendete Größe für viele neuronale Netzwerke, insbesondere in der Bildklassifikation, wie z.B. Convolutional Neural Networks (CNNs).\n",
    "\n",
    "`ToTensor()`: Hier wird das Bild in einen PyTorch-Tensor konvertiert. Die meisten neuronalen Netzwerke in PyTorch und anderen Frameworks arbeiten mit Tensoren als Eingabe.\n",
    "\n",
    "`Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Diese Transformation normalisiert die Pixelwerte des Bildes. Dies ist wichtig, um sicherzustellen, dass die Werte im Eingangsbild in einem bestimmten Bereich liegen. Die angegebenen Mittelwerte und Standardabweichungen sind typische Werte für die Normalisierung von Bildern, die auf dem ImageNet-Datensatz trainiert wurden.\n",
    "\n",
    "`RandomHorizontalFlip`: Führt mit einer Wahrscheinlichkeit von `horizontal_flip_prob` eine zufällige horizontale Spiegelung des Bildes durch.\n",
    "\n",
    "`RandomVerticalFlip`: Führt mit einer Wahrscheinlichkeit von `vertical_flip_prob` eine zufällige vertikale Spiegelung des Bildes durch.\n",
    "\n",
    "`RandomRotation`: Führt eine zufällige Rotation des Bildes um den angegebenen Winkel (`rotation_degree` Grad) durch.\n",
    "\n",
    "`ColorJitter`: Verändert die Helligkeit, den Kontrast, die Sättigung und den Farbton des Bildes zufällig, um die Farbvariationen zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "center_crpp = (500, 500)\n",
    "mean_values = [0.485, 0.456, 0.406]\n",
    "std_values = [0.229,0.224,0.225]\n",
    "\n",
    "image_transformations = Compose([\n",
    "    CenterCrop(center_crpp),\n",
    "    Resize(target_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_values,\n",
    "                std=std_values)\n",
    "])\n",
    "\n",
    "rotation_degree = 45\n",
    "horizontal_flip_prob = 0.5\n",
    "vertical_flip_prob = 0.5\n",
    "\n",
    "image_transforms_augmented = Compose([\n",
    "    ToPILImage(),\n",
    "    RandomHorizontalFlip(p=horizontal_flip_prob),\n",
    "    RandomVerticalFlip(p=vertical_flip_prob),\n",
    "    RandomRotation(degrees=rotation_degree),\n",
    "    ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    CenterCrop(center_crpp),\n",
    "    Resize(target_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_values,\n",
    "                std=std_values)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4 PreProcessing von Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionProcessor:\n",
    "    def __init__(self, max_length: int):\n",
    "        self.max_length = max_length\n",
    "        self.vocabulary = []\n",
    "        self.start_token = \"<sos>\"\n",
    "        self.stop_token = \"<eos>\"\n",
    "        self.unknown_token = \"<unk>\"\n",
    "        self.padding_token = \"<pad>\"\n",
    "        self.token_to_index = {}  \n",
    "        self.index_to_token = {}\n",
    "\n",
    "    def create_vocab(self, dataframe, caption_column: str):\n",
    "        \"\"\"\n",
    "        Create a vocabulary from a DataFrame containing captions.\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): A DataFrame containing caption data.\n",
    "            caption_column (str): The name of the column in the DataFrame that contains captions.\n",
    "        \"\"\"\n",
    "        # Join all the captions in the specified column of the DataFrame into a single string\n",
    "        all_captions = \" \".join(dataframe[caption_column].values)\n",
    "        # make all captions lowercase \n",
    "        all_captions = all_captions.lower()\n",
    "        # create a set of all unique words in the joined captions string\n",
    "        all_captions = set(all_captions.split())\n",
    "        # convert the set of unique words to a list  \n",
    "        all_captions = list(all_captions)\n",
    "        # join all the unique words into a single string separated by spaces\n",
    "        all_captions = \" \".join(all_captions) \n",
    "        # Tokenize the joined captions into words and convert them to lowercase\n",
    "        self.vocabulary = nltk.tokenize.word_tokenize(all_captions)\n",
    "        # Add special tokens to the vocabulary\n",
    "        self.vocabulary = [self.start_token, self.stop_token, self.unknown_token, self.padding_token] + self.vocabulary\n",
    "        self.token_to_index = {token: index for index, token in enumerate(self.vocabulary)}\n",
    "        self.index_to_token = {index: token for index, token in enumerate(self.vocabulary)}\n",
    "\n",
    "\n",
    "    def caption_to_tokens(self, dataframe, caption_column):\n",
    "        \"\"\"\n",
    "        Preprocess captions in a DataFrame by tokenizing and adding start and stop tokens.\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): A DataFrame containing caption data.\n",
    "            caption_column (str): The name of the column in the DataFrame that contains captions.\n",
    "        Returns:\n",
    "            pandas.DataFrame: The input DataFrame with an additional \"tokenized_caption\" column \n",
    "                              containing tokenized captions with start and stop tokens added.\n",
    "        \"\"\"\n",
    "        # Convert captions to lowercase\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[caption_column].apply(lambda x: x.lower())\n",
    "        # Split caption into tokens\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[caption_column].apply(lambda x: x.split())\n",
    "        # Add start token at the beginning\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[\"tokenized_caption\"].apply(lambda x: [self.start_token] + x)\n",
    "        # Add stop token at the end\n",
    "        dataframe[\"tokenized_caption\"] = dataframe[\"tokenized_caption\"].apply(lambda x: x + [self.stop_token])\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    def tokens_to_indices(self, tokens):\n",
    "        \"\"\"\n",
    "        Converts a list of tokens into their corresponding indices in the vocabulary.\n",
    "        Args:\n",
    "            tokens (list): List of tokens representing a caption.\n",
    "        Returns:\n",
    "            list: List of indices representing the caption.\n",
    "        \"\"\"\n",
    "        indices = [self.token_to_index[token] if token in self.vocabulary else self.token_to_index[self.unknown_token] for token in tokens]\n",
    "        # add padding to max length\n",
    "        indices_padding = indices + [self.token_to_index[self.padding_token]] * (self.max_length - len(indices))\n",
    "        # truncate to max length\n",
    "        indices_padding = indices_padding[:self.max_length]\n",
    "        return indices_padding\n",
    "    \n",
    "    def indices_to_tokens(self, indices):\n",
    "        \"\"\"\n",
    "        Converts a list of indices to their corresponding tokens.\n",
    "        Args:\n",
    "            indices (list): List of indices representing a caption.\n",
    "        Returns:\n",
    "            list: List of tokens corresponding to the input indices.\n",
    "        \"\"\"\n",
    "        tokens = [self.index_to_token[index] if index in self.index_to_token else self.unknown_token for index in indices]\n",
    "        return tokens\n",
    "\n",
    "    def tokens_to_caption(self, tokens):\n",
    "        \"\"\"\n",
    "        Converts a list of tokens into a human-readable caption, handling unknown words, padding, and maximum length.\n",
    "        Args:\n",
    "            tokens (list): List of tokens representing a caption.\n",
    "        Returns:\n",
    "            str: Human-readable caption.\n",
    "        \"\"\"\n",
    "        # Lower-case every token in the list\n",
    "        tokens = [token for token in tokens]\n",
    "        # Remove start and stop tokens\n",
    "        tokens = [token for token in tokens if token not in [self.start_token, self.stop_token, self.unknown_token, self.padding_token]]\n",
    "        # Replace unknown tokens with \"<unk>\"\n",
    "        #tokens = [token if token in self.vocabulary else self.unknown_token for token in tokens]\n",
    "        # Exclude padding tokens\n",
    "        #tokens = [token for token in tokens if token != self.padding_token]\n",
    "        # Truncate the caption to the maximum length\n",
    "        tokens = tokens[:self.max_length]\n",
    "        # Join the tokens to create the caption\n",
    "        caption = \" \".join(tokens)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>tokenized_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>[&lt;sos&gt;, A, child, in, a, pink, dress, is, clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;sos&gt;, A, girl, going, into, a, wooden, build...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>[&lt;sos&gt;, A, little, girl, climbing, into, a, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>[&lt;sos&gt;, A, little, girl, climbing, the, stairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>[&lt;sos&gt;, A, little, girl, in, a, pink, dress, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \\\n",
       "0  A child in a pink dress is climbing up a set o...   \n",
       "1              A girl going into a wooden building .   \n",
       "2   A little girl climbing into a wooden playhouse .   \n",
       "3  A little girl climbing the stairs to her playh...   \n",
       "4  A little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                   tokenized_caption  \n",
       "0  [<sos>, A, child, in, a, pink, dress, is, clim...  \n",
       "1  [<sos>, A, girl, going, into, a, wooden, build...  \n",
       "2  [<sos>, A, little, girl, climbing, into, a, wo...  \n",
       "3  [<sos>, A, little, girl, climbing, the, stairs...  \n",
       "4  [<sos>, A, little, girl, in, a, pink, dress, g...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8930\n",
      "[0, 186, 6891, 2164, 697, 4795, 2, 8038, 4437, 186, 7146, 2, 3, 3, 3, 1, 3, 3, 3, 3]\n",
      "['<sos>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<pad>', '<pad>', '<pad>', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "a group of people are standing around a table\n"
     ]
    }
   ],
   "source": [
    "caption_processor = CaptionProcessor(max_length=20)\n",
    "caption_processor.create_vocab(flicker_data, \"caption\")\n",
    "flicker_data_tokenized = caption_processor.caption_to_tokens(flicker_data, \"caption\")\n",
    "display(flicker_data_tokenized.head())\n",
    "print(\"Vocab size:\", len(caption_processor.vocabulary))\n",
    "\n",
    "# testing tokens_to_indices\n",
    "test_tokens = [\"<sos>\", \"a\", \"group\", \"of\", \"people\", \"are\", \"Ben\", \"standing\", \"around\", \"a\", \"table\", \"Ben\", \"<pad>\", \"<pad>\", \"<pad>\", \"<eos>\"]\n",
    "test_indices = caption_processor.tokens_to_indices(test_tokens)\n",
    "print(test_indices)\n",
    "\n",
    "# testing indices_to_tokens\n",
    "test_indices = [0, 476674, 476492, 476493, 476512, 476605, 2, 476596, 476476, 476674, 476103, 2, 3, 3, 3, 1, 3, 3, 3, 3]\n",
    "test_tokens = caption_processor.indices_to_tokens(test_indices)\n",
    "print(test_tokens)\n",
    "\n",
    "# testing tokens_to_caption\n",
    "test_tokens = ['<sos>', 'a', 'group', 'of', 'people', 'are', 'standing', 'around', 'a', 'table', '<unk>', '<eos>']\n",
    "test_caption = caption_processor.tokens_to_caption(test_tokens)\n",
    "print(test_caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>tokenized_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a spotted dog are fighting</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, and, a, spotted, dog, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a tri-colored dog playing with...</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, and, a, tri-colored, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a white dog with brown spots a...</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, and, a, white, dog, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>Two dogs of different breeds looking at each o...</td>\n",
       "      <td>[&lt;sos&gt;, Two, dogs, of, different, breeds, look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>Two dogs on pavement moving toward each other .</td>\n",
       "      <td>[&lt;sos&gt;, Two, dogs, on, pavement, moving, towar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40440</th>\n",
       "      <td>99679241_adc853a5c0.jpg</td>\n",
       "      <td>A grey bird stands majestically on a beach whi...</td>\n",
       "      <td>[&lt;sos&gt;, A, grey, bird, stands, majestically, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40441</th>\n",
       "      <td>99679241_adc853a5c0.jpg</td>\n",
       "      <td>A large bird stands in the water on the beach .</td>\n",
       "      <td>[&lt;sos&gt;, A, large, bird, stands, in, the, water...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40442</th>\n",
       "      <td>99679241_adc853a5c0.jpg</td>\n",
       "      <td>A tall bird is standing on the sand beside the...</td>\n",
       "      <td>[&lt;sos&gt;, A, tall, bird, is, standing, on, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40443</th>\n",
       "      <td>99679241_adc853a5c0.jpg</td>\n",
       "      <td>A water bird standing at the ocean 's edge .</td>\n",
       "      <td>[&lt;sos&gt;, A, water, bird, standing, at, the, oce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40444</th>\n",
       "      <td>99679241_adc853a5c0.jpg</td>\n",
       "      <td>A white crane stands tall as it looks out upon...</td>\n",
       "      <td>[&lt;sos&gt;, A, white, crane, stands, tall, as, it,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24270 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "5      1001773457_577c3a7d70.jpg   \n",
       "6      1001773457_577c3a7d70.jpg   \n",
       "7      1001773457_577c3a7d70.jpg   \n",
       "8      1001773457_577c3a7d70.jpg   \n",
       "9      1001773457_577c3a7d70.jpg   \n",
       "...                          ...   \n",
       "40440    99679241_adc853a5c0.jpg   \n",
       "40441    99679241_adc853a5c0.jpg   \n",
       "40442    99679241_adc853a5c0.jpg   \n",
       "40443    99679241_adc853a5c0.jpg   \n",
       "40444    99679241_adc853a5c0.jpg   \n",
       "\n",
       "                                                 caption  \\\n",
       "5             A black dog and a spotted dog are fighting   \n",
       "6      A black dog and a tri-colored dog playing with...   \n",
       "7      A black dog and a white dog with brown spots a...   \n",
       "8      Two dogs of different breeds looking at each o...   \n",
       "9        Two dogs on pavement moving toward each other .   \n",
       "...                                                  ...   \n",
       "40440  A grey bird stands majestically on a beach whi...   \n",
       "40441    A large bird stands in the water on the beach .   \n",
       "40442  A tall bird is standing on the sand beside the...   \n",
       "40443       A water bird standing at the ocean 's edge .   \n",
       "40444  A white crane stands tall as it looks out upon...   \n",
       "\n",
       "                                       tokenized_caption  \n",
       "5      [<sos>, A, black, dog, and, a, spotted, dog, a...  \n",
       "6      [<sos>, A, black, dog, and, a, tri-colored, do...  \n",
       "7      [<sos>, A, black, dog, and, a, white, dog, wit...  \n",
       "8      [<sos>, Two, dogs, of, different, breeds, look...  \n",
       "9      [<sos>, Two, dogs, on, pavement, moving, towar...  \n",
       "...                                                  ...  \n",
       "40440  [<sos>, A, grey, bird, stands, majestically, o...  \n",
       "40441  [<sos>, A, large, bird, stands, in, the, water...  \n",
       "40442  [<sos>, A, tall, bird, is, standing, on, the, ...  \n",
       "40443  [<sos>, A, water, bird, standing, at, the, oce...  \n",
       "40444  [<sos>, A, white, crane, stands, tall, as, it,...  \n",
       "\n",
       "[24270 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'val_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>tokenized_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1015118661_980735411b.jpg</td>\n",
       "      <td>A boy smiles in front of a stony wall in a city .</td>\n",
       "      <td>[&lt;sos&gt;, A, boy, smiles, in, front, of, a, ston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1015118661_980735411b.jpg</td>\n",
       "      <td>A little boy is standing on the street while a...</td>\n",
       "      <td>[&lt;sos&gt;, A, little, boy, is, standing, on, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1015118661_980735411b.jpg</td>\n",
       "      <td>A young boy runs aross the street .</td>\n",
       "      <td>[&lt;sos&gt;, A, young, boy, runs, aross, the, stree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1015118661_980735411b.jpg</td>\n",
       "      <td>A young child is walking on a stone paved stre...</td>\n",
       "      <td>[&lt;sos&gt;, A, young, child, is, walking, on, a, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1015118661_980735411b.jpg</td>\n",
       "      <td>Smiling boy in white shirt and blue jeans in f...</td>\n",
       "      <td>[&lt;sos&gt;, Smiling, boy, in, white, shirt, and, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40425</th>\n",
       "      <td>989851184_9ef368e520.jpg</td>\n",
       "      <td>A black dog has a dumbbell in his mouth .</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, has, a, dumbbell, in, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40426</th>\n",
       "      <td>989851184_9ef368e520.jpg</td>\n",
       "      <td>A black dog has a dumbbell in his mouth lookin...</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, has, a, dumbbell, in, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>989851184_9ef368e520.jpg</td>\n",
       "      <td>A black dog holding a weight in its mouth stan...</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, holding, a, weight, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>989851184_9ef368e520.jpg</td>\n",
       "      <td>A black dog holds a small white dumbbell in it...</td>\n",
       "      <td>[&lt;sos&gt;, A, black, dog, holds, a, small, white,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>989851184_9ef368e520.jpg</td>\n",
       "      <td>The black dog has a toy in its mouth and a per...</td>\n",
       "      <td>[&lt;sos&gt;, The, black, dog, has, a, toy, in, its,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8090 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "40     1015118661_980735411b.jpg   \n",
       "41     1015118661_980735411b.jpg   \n",
       "42     1015118661_980735411b.jpg   \n",
       "43     1015118661_980735411b.jpg   \n",
       "44     1015118661_980735411b.jpg   \n",
       "...                          ...   \n",
       "40425   989851184_9ef368e520.jpg   \n",
       "40426   989851184_9ef368e520.jpg   \n",
       "40427   989851184_9ef368e520.jpg   \n",
       "40428   989851184_9ef368e520.jpg   \n",
       "40429   989851184_9ef368e520.jpg   \n",
       "\n",
       "                                                 caption  \\\n",
       "40     A boy smiles in front of a stony wall in a city .   \n",
       "41     A little boy is standing on the street while a...   \n",
       "42                   A young boy runs aross the street .   \n",
       "43     A young child is walking on a stone paved stre...   \n",
       "44     Smiling boy in white shirt and blue jeans in f...   \n",
       "...                                                  ...   \n",
       "40425          A black dog has a dumbbell in his mouth .   \n",
       "40426  A black dog has a dumbbell in his mouth lookin...   \n",
       "40427  A black dog holding a weight in its mouth stan...   \n",
       "40428  A black dog holds a small white dumbbell in it...   \n",
       "40429  The black dog has a toy in its mouth and a per...   \n",
       "\n",
       "                                       tokenized_caption  \n",
       "40     [<sos>, A, boy, smiles, in, front, of, a, ston...  \n",
       "41     [<sos>, A, little, boy, is, standing, on, the,...  \n",
       "42     [<sos>, A, young, boy, runs, aross, the, stree...  \n",
       "43     [<sos>, A, young, child, is, walking, on, a, s...  \n",
       "44     [<sos>, Smiling, boy, in, white, shirt, and, b...  \n",
       "...                                                  ...  \n",
       "40425  [<sos>, A, black, dog, has, a, dumbbell, in, h...  \n",
       "40426  [<sos>, A, black, dog, has, a, dumbbell, in, h...  \n",
       "40427  [<sos>, A, black, dog, holding, a, weight, in,...  \n",
       "40428  [<sos>, A, black, dog, holds, a, small, white,...  \n",
       "40429  [<sos>, The, black, dog, has, a, toy, in, its,...  \n",
       "\n",
       "[8090 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test_data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>tokenized_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>[&lt;sos&gt;, A, child, in, a, pink, dress, is, clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;sos&gt;, A, girl, going, into, a, wooden, build...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>[&lt;sos&gt;, A, little, girl, climbing, into, a, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>[&lt;sos&gt;, A, little, girl, climbing, the, stairs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>[&lt;sos&gt;, A, little, girl, in, a, pink, dress, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "      <td>[&lt;sos&gt;, A, man, in, a, pink, shirt, climbs, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "      <td>[&lt;sos&gt;, A, man, is, rock, climbing, high, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "      <td>[&lt;sos&gt;, A, person, in, a, red, shirt, climbing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "      <td>[&lt;sos&gt;, A, rock, climber, in, a, red, shirt, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "      <td>[&lt;sos&gt;, A, rock, climber, practices, on, a, ro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8095 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \\\n",
       "0      A child in a pink dress is climbing up a set o...   \n",
       "1                  A girl going into a wooden building .   \n",
       "2       A little girl climbing into a wooden playhouse .   \n",
       "3      A little girl climbing the stairs to her playh...   \n",
       "4      A little girl in a pink dress going into a woo...   \n",
       "...                                                  ...   \n",
       "40450           A man in a pink shirt climbs a rock face   \n",
       "40451           A man is rock climbing high in the air .   \n",
       "40452  A person in a red shirt climbing up a rock fac...   \n",
       "40453                    A rock climber in a red shirt .   \n",
       "40454  A rock climber practices on a rock climbing wa...   \n",
       "\n",
       "                                       tokenized_caption  \n",
       "0      [<sos>, A, child, in, a, pink, dress, is, clim...  \n",
       "1      [<sos>, A, girl, going, into, a, wooden, build...  \n",
       "2      [<sos>, A, little, girl, climbing, into, a, wo...  \n",
       "3      [<sos>, A, little, girl, climbing, the, stairs...  \n",
       "4      [<sos>, A, little, girl, in, a, pink, dress, g...  \n",
       "...                                                  ...  \n",
       "40450  [<sos>, A, man, in, a, pink, shirt, climbs, a,...  \n",
       "40451  [<sos>, A, man, is, rock, climbing, high, in, ...  \n",
       "40452  [<sos>, A, person, in, a, red, shirt, climbing...  \n",
       "40453  [<sos>, A, rock, climber, in, a, red, shirt, ....  \n",
       "40454  [<sos>, A, rock, climber, practices, on, a, ro...  \n",
       "\n",
       "[8095 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = flicker_data_tokenized[\"image\"].unique()\n",
    "\n",
    "train_images, val_test_images = train_test_split(images, test_size=0.4, random_state=42)\n",
    "val_images, test_images = train_test_split(val_test_images, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = flicker_data_tokenized[flicker_data_tokenized[\"image\"].isin(train_images)]\n",
    "val_df = flicker_data_tokenized[flicker_data_tokenized[\"image\"].isin(val_images)]\n",
    "test_df = flicker_data_tokenized[flicker_data_tokenized[\"image\"].isin(test_images)]\n",
    "\n",
    "display(\"train_data\", train_df)\n",
    "display(\"val_data\", val_df)\n",
    "display(\"test_data\", test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data 24270\n",
      "val_data 8090\n",
      "test_data 8095\n",
      "image_train torch.Size([3, 224, 224])\n",
      "caption_train torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "train_data = Flicker8kDataset(train_df, image_path, image_transformations, caption_processor)\n",
    "val_data = Flicker8kDataset(val_df, image_path, image_transformations, caption_processor)\n",
    "test_data = Flicker8kDataset(test_df, image_path, image_transformations, caption_processor)\n",
    "\n",
    "print(\"train_data\", len(train_data))\n",
    "print(\"val_data\", len(val_data))\n",
    "print(\"test_data\", len(test_data))\n",
    "image_train, caption_train = train_data[1]\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=False, num_workers=0, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, pin_memory=False, num_workers=0, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, pin_memory=False, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 12135\n",
      "Number of batches in val_loader: 4045\n",
      "Number of batches in test_loader: 4048\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in val_loader: {len(val_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train torch.Size([2, 3, 224, 224])\n",
      "caption_train torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "# get dimension of the data, and the first data\n",
    "image_train, caption_train = next(iter(val_loader))\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 6 CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # define embedding dimension\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # Use ResNet-18 with pre-trained weights\n",
    "\n",
    "        # freeze all layers except the last linear layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "\n",
    "        # overwrite the last layer\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, self.embedding_dim), \n",
    "                                       nn.BatchNorm1d(self.embedding_dim, momentum=0.01))\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7 LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        # define the properties\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        # define the layers\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Decode feature vectors and generates captions.\n",
    "        Args:\n",
    "            features (torch.Tensor): Tensor of extracted feature vectors from images.\n",
    "            captions (torch.Tensor): Tensor of captions.\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of predicted captions.\n",
    "        \"\"\"\n",
    "        # Remove end token from captions\n",
    "        captions = captions[:, :-1]\n",
    "        # Embed the captions\n",
    "        embeddings = self.embed(captions)\n",
    "        # Concatenate the feature vectors and embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        # Pass the embeddings through the LSTM cells\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        # Reshape outputs to be (batch_size * sequence_length, hidden_size)\n",
    "        outputs = hiddens.reshape(-1, hiddens.size(2))\n",
    "        # Pass the outputs through the linear layer\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def greedy_sample(self, features, states=None, max_length=20):\n",
    "        batch_size = features.size(0)  # Get the batch size\n",
    "        sampled_ids = [[] for _ in range(batch_size)]  # List to store the sampled captions for each image in the batch\n",
    "        \n",
    "        # Prepare the initial input for LSTM, which is the features tensor\n",
    "        inputs = features.unsqueeze(1)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            hiddens, states = self.lstm(inputs, states)  # Pass the input through LSTM\n",
    "            outputs = self.linear(hiddens.squeeze(1))  # Pass the LSTM outputs through the linear layer\n",
    "\n",
    "            predicted = outputs.argmax(1)  # Get the predicted word indices\n",
    "            for j in range(batch_size):\n",
    "                # break of eos token \n",
    "                if predicted[j].item() == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    sampled_ids[j].append(predicted[j].item())  # Append the predicted word index to the respective caption list\n",
    "\n",
    "            inputs = self.embed(predicted)  # Prepare the input for the next time step\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        return sampled_ids\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the linear layer.\n",
    "        \"\"\"\n",
    "        self.linear.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.linear.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8 Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, num_layers=1):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = EncoderCNN(embedding_dim)\n",
    "        self.decoder = DecoderLSTM(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        for images, captions in tqdm(train_loader, desc=\"Training\"):\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self(images, captions)\n",
    "            targets = pack_padded_sequence(captions, [len(caption) for caption in captions], batch_first=True, enforce_sorted=False)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader, criterion, device):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, captions in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images, captions = images.to(device), captions.to(device)\n",
    "                outputs = self(images, captions)\n",
    "\n",
    "                targets = pack_padded_sequence(captions, [len(caption) for caption in captions], batch_first=True, enforce_sorted=False)[0]\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): EncoderCNN(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderLSTM(\n",
       "    (embed): Embedding(8930, 128)\n",
       "    (lstm): LSTM(128, 128, batch_first=True)\n",
       "    (linear): Linear(in_features=128, out_features=8930, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "hidden_size = 128\n",
    "vocab_size = len(caption_processor.vocabulary)\n",
    "num_layers = 1\n",
    "\n",
    "# Instantiate the encoder_decoder model\n",
    "encoder_decoder = EncoderDecoder(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "# Move the model to a device\n",
    "device = device_setup.device\n",
    "encoder_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train torch.Size([2, 3, 224, 224])\n",
      "caption_train torch.Size([2, 20])\n",
      "outputs torch.Size([40, 8930])\n",
      "outputs tensor([[ 0.4985,  0.4464, -0.1168,  ...,  0.2760, -0.2345, -1.2967],\n",
      "        [ 0.6662,  0.5291, -0.2665,  ...,  0.4679, -0.1075, -0.5587],\n",
      "        [ 0.6886,  0.8839,  0.1018,  ...,  0.7075, -0.6007, -0.8527],\n",
      "        ...,\n",
      "        [ 0.3658,  0.3950, -0.6639,  ...,  0.0462,  0.0544,  0.0748],\n",
      "        [ 0.1653,  0.4539,  0.1088,  ...,  0.2787,  0.1996, -0.1900],\n",
      "        [ 0.9813, -0.7274, -0.0372,  ...,  0.3013,  0.2359,  0.0343]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "sampled_ids [[7170, 3861, 1519, 1763, 3424, 777, 3764, 7382, 5056, 817, 5559, 3761, 6093, 2176, 3757, 8678, 7063, 2927, 282, 2192], [748, 3200, 5804, 3682, 3219, 1218, 8205, 8786, 5326, 8136, 4598, 1682, 5630, 1861, 4844, 6954, 6383, 8903, 8392, 675]]\n"
     ]
    }
   ],
   "source": [
    "# test the encoder_decoder\n",
    "image_train, caption_train = next(iter(train_loader))\n",
    "image_train = image_train.to(device_setup.device)\n",
    "caption_train = caption_train.to(device_setup.device)\n",
    "\n",
    "print(\"image_train\", image_train.shape)\n",
    "print(\"caption_train\", caption_train.shape)\n",
    "outputs = encoder_decoder(image_train, caption_train)\n",
    "# get features\n",
    "features = encoder_decoder.encoder(image_train)\n",
    "\n",
    "print(\"outputs\", outputs.shape)\n",
    "print(\"outputs\", outputs)\n",
    "\n",
    "# predict the caption\n",
    "sampled_ids = encoder_decoder.decoder.greedy_sample(features)\n",
    "print(\"sampled_ids\", sampled_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(encoder_decoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = encoder_decoder.train_one_epoch(train_loader, criterion, optimizer, device)\n",
    "    val_loss = encoder_decoder.validate(val_loader, criterion, device)\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9 Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder_decoder model\n",
    "torch.save(encoder_decoder.state_dict(), \"models/encoder_decoder_model_5_epoch_train_val_set.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder.load_state_dict(torch.load(\"models/encoder_decoder_model_5_epoch_train_val_set.pth\"))\n",
    "encoder_decoder.to(device_setup.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10 Evalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(data_loader, encoder_decoder, caption_processor, device, image_path, num_images=8):\n",
    "    encoder_decoder.eval()\n",
    "\n",
    "    # Get a batch of data\n",
    "    for images, _ in iter(data_loader):\n",
    "        break\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Generate captions\n",
    "    predicted_captions = []\n",
    "    for i in range(0, num_images * 5, 5):  # Iterating over every 5th image\n",
    "        if i >= len(images):\n",
    "            break\n",
    "        image = images[i]\n",
    "        features = encoder_decoder.encoder(image.unsqueeze(0))\n",
    "        sampled_ids = encoder_decoder.decoder.greedy_sample(features)\n",
    "        tokens = [caption_processor.indices_to_tokens(ids) for ids in sampled_ids]\n",
    "        captions = [caption_processor.tokens_to_caption(token_list) for token_list in tokens]\n",
    "        predicted_captions.append(captions)\n",
    "\n",
    "    # Ensure we have enough rows in the subplot\n",
    "    num_rows = min(num_images, len(predicted_captions))\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, 1, figsize=(10, 20))\n",
    "    if num_rows == 1:\n",
    "        axs = [axs]  # Make axs iterable even if it's a single Axes object\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        # Adjust the index to account for the 5-image groups\n",
    "        adjusted_index = i * 5\n",
    "        image_file = os.path.join(image_path, data_loader.dataset.dataframe.iloc[adjusted_index][\"image\"])\n",
    "        if not os.path.exists(image_file):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(image_file)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "        true_captions = data_loader.dataset.dataframe[data_loader.dataset.dataframe[\"image\"] == data_loader.dataset.dataframe.iloc[adjusted_index][\"image\"]][\"caption\"].tolist()\n",
    "        true_captions_text = \"\\n\".join(true_captions)\n",
    "\n",
    "        ax.set_title(f'Predicted: {predicted_captions[i][0]}\\nTrue: {true_captions_text}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_results(test_loader, encoder_decoder, caption_processor, device, image_path, num_images=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
