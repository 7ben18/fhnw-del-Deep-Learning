{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; float: right; margin: 10px; margin-right: 5%;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/FHNW_Logo.svg/2560px-FHNW_Logo.svg.png\" width=\"500\" style=\"float: left; filter: invert(50%);\"/>\n",
    "</div>\n",
    "\n",
    "<h1 style=\"text-align: left; margin-top: 10px; float: left; width: 60%;\">\n",
    "    del Mini-Challenge 2 <br> \n",
    "</h1>\n",
    "\n",
    "<p style=\"clear: both; text-align: left;\">\n",
    "    Bearbeitet durch Si Ben Tran im HS 2023.<br>Bachelor of Science FHNW in Data Science.\n",
    "</p>\n",
    "\n",
    "\n",
    "Ziel:  \n",
    "Vertiefung in ein eher aktuelles Paper aus der Forschung und Umsetzung eines darin beschriebenen oder verwandten Tasks - gemäss Vereinbarung mit dem Fachcoach. \n",
    "\n",
    "Beispiel:  \n",
    "Implementiere, trainiere und validiere ein Deep Learning Modell für Image Captioning wie beschrieben im Paper Show and Tell.\n",
    "\n",
    "Zeitlicher Rahmen:  \n",
    "Wird beim Schritt 1 verbindlich festgelegt.\n",
    "\n",
    "Beurteilung:  \n",
    "Beurteilt wird auf Basis des abgegebenen Notebooks:  \n",
    "•\tVollständige und korrekte Umsetzung der vereinbarten Aufgabestellung.  \n",
    "•\tKlare, gut-strukturierte Umsetzung.   \n",
    "•\tSchlüssige Beschreibung und Interpretation der Ergebnisse. Gut gewählte und gut kommentierten Plots und Tabellen.  \n",
    "•\tVernünftiger Umgang mit (Computing-)Ressourcen.  \n",
    "•\tVerständliche Präsentation der Ergebnisse.  \n",
    "\n",
    "Referenzen, Key Words  \n",
    "•\tWord Embedding (z.B. word2vec, glove), um Wörter in numerische Vektoren in einem geeignet dimensionierten Raum zu mappen. Siehe z.B. Andrew Ng, Coursera: [Link](https://www.coursera.org/lecture/nlp-sequence-models/learning-word-embeddings-APM5s)      \n",
    "•\tBild Embedding mittels vortrainierten (evt. retrained) Netzwerken wie beispielsweise ResNet, GoogLeNet, EfficientNet oder ähnlich Transfer-Learning.  \n",
    "•\tSeq2Seq Models bekannt für Sprach-Übersetzung. \n",
    "\n",
    "Daten:   \n",
    "•\tGemäss Vereinbarung (für Captioning: [Flickr8k-Daten](https://www.kaggle.com/adityajn105/flickr8k/activity)).\n",
    "\n",
    "•\tAbsprache/Beschluss mit Coach und Beschluss, was evaluiert werden soll.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# path setup\n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "# Data Science Libraries\n",
    "import tqdm \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import cpu_count\n",
    "import random\n",
    "\n",
    "\n",
    "# Text Processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from typing import List\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models import ResNet18_Weights, DenseNet121_Weights\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor, ToPILImage, Normalize\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, RandomVerticalFlip, ColorJitter\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Custom Modules\n",
    "from src.gpu_setup import DeviceSetup\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_setup = DeviceSetup(seed=42)\n",
    "device_setup.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2 Daten\n",
    "Wir erkennen bei der Spalte *image*, das ein jpg. Bilddatei mehrere *catpion* hat.\n",
    "\n",
    "Bei der Visualisierung der Bilder erkenne wir:\n",
    "- Personen oder Tiere\n",
    "- Unterschiedliche Grössen\n",
    "- Unterschiedliche Auflösung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExplorer:\n",
    "    def __init__(self, image_path, captions_path):\n",
    "        self.image_path = image_path\n",
    "        self.data = pd.read_csv(captions_path)\n",
    "\n",
    "\n",
    "    def _get_image_unique(self):\n",
    "        \"\"\"\n",
    "        This method returns a list of unique image IDs.\n",
    "        \"\"\"\n",
    "        image_unique = self.data['image'].unique()\n",
    "        return image_unique\n",
    "    \n",
    "    def _get_word_counts(self):\n",
    "        \"\"\"\n",
    "        This method returns a list of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self.data['caption'].apply(str.split).apply(len)\n",
    "        return word_counts\n",
    "    \n",
    "    def _read_image(self, image_id):\n",
    "        \"\"\"\n",
    "        This method reads an image from a specific path and returns the image object.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.image_path + \"/\" + image_id)\n",
    "        return image\n",
    "\n",
    "    def _get_captions(self, image_id):\n",
    "        \"\"\"\n",
    "        This method retrieves the captions associated with an image ID from the data dictionary.\n",
    "        \"\"\"\n",
    "        captions = []\n",
    "        for i in range(len(self.data)):\n",
    "            if self.data['image'][i] == image_id:\n",
    "                captions.append(self.data['caption'][i])\n",
    "        captions = '\\n'.join(captions)\n",
    "        return captions\n",
    "\n",
    "    def plot_n_m_image_caption(self, n, m):\n",
    "        \"\"\"\n",
    "        This method plots a grid of n x m images along with their captions.\n",
    "        \"\"\"\n",
    "        image_unique = self._get_image_unique()\n",
    "        fig, ax = plt.subplots(n, m, figsize=(16, 20))\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                index = np.random.randint(0, len(image_unique))\n",
    "                image_id = image_unique[index]\n",
    "                image = self._read_image(image_id)\n",
    "                captions = self._get_captions(image_id)\n",
    "                ax[i, j].imshow(np.asarray(image))\n",
    "                ax[i, j].set_title(captions)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_image_size(self):\n",
    "        \"\"\"\n",
    "        This method plots a grid of n x m images along with their captions.\n",
    "        \"\"\"\n",
    "        image_unique = self._get_image_unique()\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        # set range of x and y axis\n",
    "        ax.set_xlabel('width of image')\n",
    "        ax.set_ylabel('height of image')\n",
    "        for i in range(len(image_unique)):\n",
    "            image_id = image_unique[i]\n",
    "            image = self._read_image(image_id)\n",
    "            width, height = image.size\n",
    "            ax.scatter(width, height)\n",
    "        ax.set_title('Distribution size of images')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # plot caption distribution word length\n",
    "    def plot_caption_distribution(self):\n",
    "        \"\"\"\n",
    "        This method plots the distribution of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.hist(word_counts, bins=25, color = 'limegreen', edgecolor='black', linewidth=1.2)\n",
    "        plt.title(\"Distribution of Number of Words per Caption\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "    # get statistical summary of caption distribution word length\n",
    "    def get_caption_distribution(self):\n",
    "        \"\"\"\n",
    "        This method prints the statistical summary of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        print(word_counts.describe(percentiles=[0.25, 0.5, 0.75, 0.95]))\n",
    "\n",
    "    def plot_caption_ecdf(self):\n",
    "        \"\"\"\n",
    "        This method plots the ECDF of the number of words per caption.\n",
    "        \"\"\"\n",
    "        word_counts = self._get_word_counts()\n",
    "        word_counts_sorted = word_counts.sort_values()\n",
    "        y = np.arange(1, len(word_counts_sorted) + 1) / len(word_counts_sorted)  \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(word_counts_sorted, y, color='limegreen')\n",
    "        plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "        plt.axvline(x=19, color='r', linestyle='-')\n",
    "        plt.xticks(np.arange(np.min(word_counts_sorted), np.max(word_counts_sorted), 1.0))\n",
    "        plt.title(\"ECDF of Number of Words per Caption\")\n",
    "        plt.xlabel(\"Number of Words\")\n",
    "        plt.ylabel(\"Proportion\")\n",
    "        plt.show()\n",
    "\n",
    "    # plot most commen words\n",
    "    def plot_most_common_words(self):\n",
    "        \"\"\"\n",
    "        This method plots the most common words in the captions.\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        word_counts = self.data['caption'].apply(str.split).apply(Counter).sum()\n",
    "        word_counts = pd.DataFrame.from_dict(word_counts, orient='index').reset_index()\n",
    "        word_counts.columns = ['word', 'count']\n",
    "        word_counts = word_counts.sort_values(by='count', ascending=False)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.bar(word_counts['word'][:20], word_counts['count'][:20], color = 'limegreen', edgecolor='black', linewidth=1.2)\n",
    "        plt.title(\"Most Common Words in Captions\")\n",
    "        plt.xlabel(\"Words\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/Flickr8K/images/\"\n",
    "captions_path = \"data/Flickr8K/captions.txt\"\n",
    "\n",
    "flicker_data_explorer = DataExplorer(image_path, captions_path)\n",
    "flicker_data = flicker_data_explorer.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flicker_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualisierungen der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_n_m_image_caption(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Grössen der Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_image_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Caption Länge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_caption_distribution()\n",
    "#flicker_data_explorer.plot_caption_ecdf()\n",
    "#flicker_data_explorer.get_caption_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Häufigste Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flicker_data_explorer.plot_most_common_words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3 Preprocessing der Bilder\n",
    "\n",
    "Wir werden Die Bilder wie folgt vorbereiten, damit das Model die Bilder verarbeiten kann: \n",
    "\n",
    "`ToPILImage()`: Dieser Schritt konvertiert das Eingabebild in ein PIL (Python Imaging Library) Bildformat. Dies ist erforderlich, wenn das Eingabebild nicht bereits im PIL-Format vorliegt.\n",
    "\n",
    "`CenterCrop((500, 500))`: Hier wird das Bild auf eine Größe von 500x500 Pixel zentriert zugeschnitten. Dies ist nützlich, um das Bild auf eine bestimmte Größe zu bringen und sicherzustellen, dass wichtige Merkmale in der Mitte erhalten bleiben.\n",
    "\n",
    "`Resize((224, 224))`: Das Bild wird auf eine Größe von 224x224 Pixel skaliert. Dies ist eine häufig verwendete Größe für viele neuronale Netzwerke, insbesondere in der Bildklassifikation, wie z.B. Convolutional Neural Networks (CNNs).\n",
    "\n",
    "`ToTensor()`: Hier wird das Bild in einen PyTorch-Tensor konvertiert. Die meisten neuronalen Netzwerke in PyTorch und anderen Frameworks arbeiten mit Tensoren als Eingabe.\n",
    "\n",
    "`Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`: Diese Transformation normalisiert die Pixelwerte des Bildes. Dies ist wichtig, um sicherzustellen, dass die Werte im Eingangsbild in einem bestimmten Bereich liegen. Die angegebenen Mittelwerte und Standardabweichungen sind typische Werte für die Normalisierung von Bildern, die auf dem ImageNet-Datensatz trainiert wurden.\n",
    "\n",
    "`RandomHorizontalFlip`: Führt mit einer Wahrscheinlichkeit von `horizontal_flip_prob` eine zufällige horizontale Spiegelung des Bildes durch.\n",
    "\n",
    "`RandomVerticalFlip`: Führt mit einer Wahrscheinlichkeit von `vertical_flip_prob` eine zufällige vertikale Spiegelung des Bildes durch.\n",
    "\n",
    "`RandomRotation`: Führt eine zufällige Rotation des Bildes um den angegebenen Winkel (`rotation_degree` Grad) durch.\n",
    "\n",
    "`ColorJitter`: Verändert die Helligkeit, den Kontrast, die Sättigung und den Farbton des Bildes zufällig, um die Farbvariationen zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "center_crpp = (500, 500)\n",
    "mean_values = [0.485, 0.456, 0.406]\n",
    "std_values = [0.229,0.224,0.225]\n",
    "\n",
    "image_transformations = Compose([\n",
    "    CenterCrop(center_crpp),\n",
    "    Resize(target_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_values,\n",
    "                std=std_values)\n",
    "])\n",
    "\n",
    "rotation_degree = 45\n",
    "horizontal_flip_prob = 0.5\n",
    "vertical_flip_prob = 0.5\n",
    "\n",
    "image_transforms_augmented = Compose([\n",
    "    RandomHorizontalFlip(p=horizontal_flip_prob),\n",
    "    RandomVerticalFlip(p=vertical_flip_prob),\n",
    "    RandomRotation(degrees=rotation_degree),\n",
    "    CenterCrop(center_crpp),\n",
    "    Resize(target_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean_values,\n",
    "                std=std_values)\n",
    "])\n",
    "\n",
    "\n",
    "image_inverse_transformations = Compose([\n",
    "     Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                 std=[1/0.229, 1/0.224, 1/0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Test der Transformationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transformation on sample image\n",
    "image_id = flicker_data['image'][0]\n",
    "image = flicker_data_explorer._read_image(image_id)\n",
    "image_transformed = image_transformations(image)\n",
    "image_transformed_augmented = image_transforms_augmented(image)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 8))\n",
    "ax[0].imshow(np.asarray(image))\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(np.transpose(image_transformed, (1, 2, 0)))\n",
    "ax[1].set_title('Transformed Image')\n",
    "ax[2].imshow(np.transpose(image_transformed_augmented, (1, 2, 0)))\n",
    "ax[2].set_title('Augmented Image')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4 PreProcessing von Captions\n",
    "Die gezeigte Python-Klasse `PreprocessCaption` dient der Vorverarbeitung von Bildunterschriften (captions) in natürlicher Sprache.\n",
    "\n",
    "Konstruktor `__init__`:\n",
    "\n",
    "Der Konstruktor dieser Klasse wird beim Erstellen eines Objekts aufgerufen und hat einen optionalen Parameter `threshold`, der auf den Wert 2 eingestellt ist. Dieser Schwellenwert dient dazu, die Wörter in der Vokabelliste zu filtern. Die Klasse hat außerdem einige Instanzvariablen, darunter `threshold`, `vocab`, `word_to_index`, `index_to_word`, `freq_dist` und `special_tokens`. \n",
    "\n",
    "- `threshold`: Der Schwellenwert zur Auswahl der Wörter im Vokabular basierend auf ihrer Häufigkeit.\n",
    "- `vocab`: Eine Liste, die das Vokabular enthält.\n",
    "- `word_to_index`: Ein Wörterbuch, das Wörter auf ihre Indexpositionen im Vokabular abbildet.\n",
    "- `index_to_word`: Ein Wörterbuch, das Indexpositionen auf die entsprechenden Wörter im Vokabular abbildet.\n",
    "- `freq_dist`: Eine Instanz von `FreqDist` aus der NLTK-Bibliothek, die die Häufigkeit der Wörter im Text speichert.\n",
    "- `special_tokens`: Eine Liste von speziellen Tokens wie '<START>', '<END>', '<PAD>' und '<UNK>', die oft in NLP-Aufgaben verwendet werden.\n",
    "\n",
    "`create_vocabulary`:\n",
    "Diese Methode erstellt das Vokabular basierend auf den übergebenen Daten in Form eines Pandas DataFrame. Sie tokenisiert zuerst alle Bildunterschriften, filtert Wörter nach ihrer Häufigkeit unter Verwendung des Schwellenwerts und fügt schließlich die speziellen Tokens zum Vokabular hinzu. Das Ergebnis wird in den Instanzvariablen `vocab`, `word_to_index` und `index_to_word` gespeichert.\n",
    "\n",
    "`get_vocab`:\n",
    "Diese Methode gibt das Vokabular als Liste von Wörtern zurück.\n",
    "\n",
    "`get_vocab_size`:\n",
    "Diese Methode gibt die Größe des Vokabulars zurück, dh die Anzahl der eindeutigen Wörter im Vokabular.\n",
    "\n",
    "`get_word_to_index`:\n",
    "Diese Methode gibt das Wörterbuch zurück, das Wörter auf ihre Indexpositionen im Vokabular abbildet.\n",
    "\n",
    "`get_index_to_word`:\n",
    "Diese Methode gibt das Wörterbuch zurück, das Indexpositionen auf die entsprechenden Wörter im Vokabular abbildet.\n",
    "\n",
    "`caption_to_tokens`:\n",
    "Diese Methode konvertiert eine Bildunterschrift in eine Liste von Token-Indizes unter Verwendung des erstellten Vokabulars. Die Tokens werden in der Reihenfolge '<START>', gefolgt von den Token-Indizes der Wörter in der Bildunterschrift und schließlich '<END>' zurückgegeben.\n",
    "\n",
    "`tokens_to_caption`:\n",
    "Diese Methode konvertiert eine Liste von Token-Indizes zurück in eine menschenlesbare Bildunterschrift, indem sie die Wörter aus dem Vokabular extrahiert und sie in der richtigen Reihenfolge anordnet, bis '<END>' erreicht wird. Dabei werden spezielle Tokens wie '<START>' und '<END>' ignoriert.\n",
    "\n",
    "Die Klasse `PreprocessCaption` ermöglicht es, Textdaten für die Verwendung in Machine Learning- oder Deep Learning-Modellen vorzubereiten, insbesondere für Aufgaben im Bereich des maschinellen Sehens (Computer Vision), bei denen Bildunterschriften verarbeitet werden müssen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessCaption:\n",
    "    def __init__(self, threshold: int = 0):\n",
    "        self.threshold = threshold\n",
    "        self.vocab = []\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.freq_dist = None\n",
    "        self.special_tokens = ['<START>', '<END>', '<PAD>', '<UNK>']\n",
    "\n",
    "    def get_vocab(self) -> List[str]:\n",
    "        return self.vocab\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_word_to_index(self) -> dict:\n",
    "        return self.word_to_index\n",
    "    \n",
    "    def get_index_to_word(self) -> dict:\n",
    "        return self.index_to_word\n",
    "\n",
    "    def create_vocabulary(self, data: pd.DataFrame) -> None:\n",
    "        all_captions = \" \".join(data[\"caption\"].values)\n",
    "        words = word_tokenize(all_captions.lower())\n",
    "        words = [word for word in words if word.isalpha()]\n",
    "        self.freq_dist = FreqDist(words)\n",
    "\n",
    "        self.vocab = [word for word, freq in self.freq_dist.items() if freq >= self.threshold]\n",
    "        self.vocab = self.special_tokens + self.vocab\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.index_to_word = {idx: word for idx, word in enumerate(self.vocab)}\n",
    "\n",
    "    def caption_to_tokens(self, caption: str) -> List[int]:\n",
    "        if not caption:\n",
    "            return []\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "        tokenized_caption = [self.word_to_index.get('<START>')]\n",
    "    \n",
    "        for token in tokens:\n",
    "            tokenized_caption.append(self.word_to_index.get(token, self.word_to_index.get('<UNK>')))\n",
    "    \n",
    "        tokenized_caption.append(self.word_to_index.get('<END>'))\n",
    "        return tokenized_caption\n",
    "\n",
    "    def tokens_to_caption(self, tokens: List[int]) -> str:\n",
    "        words = []\n",
    "        for idx in tokens:\n",
    "            word = self.index_to_word.get(idx)\n",
    "            if word == '<END>':\n",
    "                break\n",
    "            if word and word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Test Preprocessing der Captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VocabularyBuilder\n",
    "caption_info = PreprocessCaption(threshold=0)\n",
    "\n",
    "# Create vocabulary\n",
    "caption_info.create_vocabulary(flicker_data)\n",
    "\n",
    "# Get vocabulary and its size\n",
    "vocab = caption_info.get_vocab()\n",
    "vocab_size = caption_info.get_vocab_size()\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print()\n",
    "\n",
    "# Get word_to_index and index_to_word\n",
    "word_to_index = caption_info.get_word_to_index()\n",
    "index_to_word = caption_info.get_index_to_word()\n",
    "\n",
    "print(\"Word to Index:\", word_to_index)\n",
    "print(\"Index to Word:\", index_to_word)\n",
    "print()\n",
    "\n",
    "# Call caption_to_tokens and tokens_to_caption methods\n",
    "caption = \"A black 43$#1 dog is running towards 34 pebsi the 0.5 old grey 18 cat\"\n",
    "tokens = caption_info.caption_to_tokens(caption)\n",
    "print(\"Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "reconstructed_caption = caption_info.tokens_to_caption(tokens)\n",
    "print(\"Reconstructed Caption:\", reconstructed_caption)\n",
    "print()\n",
    "\n",
    "# get token from PAD\n",
    "pad_token = word_to_index.get('<PAD>')\n",
    "print(\"PAD Token:\", pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Test Preprocessing der Captions mit Trim und Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_to_pad = \"a girl running around a tree\"\n",
    "caption_to_trim = \"A girl running around a tree while playing with a dog and cat in the great big field of grass and flowers next to the river\"\n",
    "\n",
    "print(\"Caption to Pad:\", caption_to_pad)\n",
    "print(\"Caption to Trim:\", caption_to_trim)\n",
    "print()\n",
    "\n",
    "max_length = 20\n",
    "caption_to_pad_tokens = caption_info.caption_to_tokens(caption_to_pad)\n",
    "caption_to_trim_tokens = caption_info.caption_to_tokens(caption_to_trim)\n",
    "\n",
    "print(\"Caption to Pad Tokenized:\", caption_to_pad_tokens)\n",
    "print(\"Caption to Trim Tokenized:\", caption_to_trim_tokens)\n",
    "print()\n",
    "\n",
    "padded_caption = caption_to_pad_tokens + [pad_token] * (max_length - len(caption_to_pad_tokens))\n",
    "trimmed_caption = caption_to_trim_tokens[:max_length]\n",
    "\n",
    "print(f\"Padded Caption: {padded_caption},\\n Length: {len(padded_caption)}\")\n",
    "print(f\"Trimmed Caption: {trimmed_caption},\\n Length: {len(trimmed_caption)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5 Dataset Vorbereitung\n",
    "\n",
    "Beschreibung der ImageCaptionDataset-Klasse\n",
    "\n",
    "Die `ImageCaptionDataset`-Klasse ist eine Implementierung eines PyTorch-Datensatzes, der dazu dient, Daten für das Show-and-Tell-Modell zur Bildbeschreibung (Image Captioning) bereitzustellen. Hier ist eine Beschreibung der verschiedenen Aspekte dieser Klasse:\n",
    "\n",
    "- **`__init__(self, dataframe, image_path, vocab_size, caption_processor, transform=None, max_length=20)`**: Dies ist der Konstruktor der Klasse. Hier werden die grundlegenden Parameter und Attribute des Datensatzes initialisiert:\n",
    "   - `dataframe`: Ein Pandas-Datenrahmen, der Informationen über Bilder und zugehörige Bildunterschriften enthält.\n",
    "   - `image_path`: Der Pfad zum Verzeichnis, in dem die Bilder gespeichert sind.\n",
    "   - `vocab_size`: Die Größe des Vokabulars, das für die Textverarbeitung verwendet wird.\n",
    "   - `caption_processor`: Ein Prozessor oder Tokenizer, der dazu dient, Bildunterschriften in tokenisierte Form umzuwandeln.\n",
    "   - `transform`: Eine optionale Transformation, die auf Bilder angewendet werden kann (z. B. Normalisierung oder Umwandlung in Tensoren).\n",
    "   - `max_length`: Die maximale Länge der tokenisierten Bildunterschriften, die behalten oder beschnitten werden sollen.\n",
    "\n",
    "- **`__len__(self)`**: Diese Methode gibt die Gesamtanzahl der Beispiele im Datensatz zurück, was der Länge des übergebenen Datenrahmens entspricht.\n",
    "\n",
    "- **`__getitem__(self, idx)`**: Diese Methode wird aufgerufen, um ein bestimmtes Beispiel im Datensatz anhand seines Index `idx` abzurufen. Sie führt die folgenden Schritte aus:\n",
    "   - Liest den Dateinamen des Bildes aus dem Datenrahmen ab und öffnet das Bild.\n",
    "   - Wendet eine optionale Transformation auf das Bild an.\n",
    "   - Extrahiert die zugehörige Bildunterschrift aus dem Datenrahmen.\n",
    "   - Tokenisiert die Bildunterschrift mithilfe des `caption_processor`.\n",
    "   - Beschränkt die Länge der tokenisierten Bildunterschrift auf `max_length` durch Auffüllen oder Beschneiden.\n",
    "   - Wandelt die tokenisierte Bildunterschrift in einen Tensor um und gibt sowohl das Bild als auch den Tensor zurück.\n",
    "\n",
    "Das Ziel dieser Klasse ist es, Bild-Text-Paare aus einem Datenrahmen zu erstellen und sie in einer Form bereitzustellen, die von einem Show-and-Tell-Modell für die Bildbeschreibung verwendet werden kann. Das Bild wird in der Regel als Input für das CNN (Convolutional Neural Network) und die tokenisierte Bildunterschrift als Ziel für das RNN (Recurrent Neural Network) des Modells verwendet. Dies ermöglicht dem Modell, Bilder in natürlicher Sprache zu beschreiben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_path, vocab_size, caption_processor, transform=None, max_length=20):\n",
    "        self.dataframe = dataframe\n",
    "        self.images_path = image_path\n",
    "        self.vocab_size = vocab_size\n",
    "        self.caption_processor = caption_processor\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.dataframe.iloc[idx]['image']\n",
    "        image = Image.open(os.path.join(self.images_path, image_name)).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = self.dataframe.iloc[idx]['caption']\n",
    "        caption_tokens = self.caption_processor.caption_to_tokens(caption)\n",
    "        caption_tokens = caption_tokens[:self.max_length]\n",
    "\n",
    "        # Pad the caption tokens to the maximum length or trim \n",
    "        if len(caption_tokens) < self.max_length:\n",
    "            # padding\n",
    "            caption_tokens = caption_tokens + [self.caption_processor.word_to_index[\"<PAD>\"]] * (self.max_length - len(caption_tokens))\n",
    "        else:\n",
    "            # trimming\n",
    "            caption_tokens = caption_tokens[:self.max_length]\n",
    "\n",
    "        caption_tokens = torch.tensor(caption_tokens, dtype=torch.long)\n",
    "\n",
    "        return image, caption_tokens\n",
    "    \n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, image_path, vocab_size, caption_info, image_transformations, batch_size=64):\n",
    "        self.image_path = image_path\n",
    "        self.vocab_size = vocab_size\n",
    "        self.caption_info = caption_info\n",
    "        self.image_transformations = image_transformations\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def split_dataframe_by_images(self, dataframe, val_test_size=0.4, random_state=42, get_len=False):\n",
    "        unique_images = dataframe[\"image\"].unique()\n",
    "\n",
    "        train_images, val_test_images = train_test_split(unique_images, test_size=val_test_size, random_state=random_state)\n",
    "        val_images, test_images = train_test_split(val_test_images, test_size=0.5, random_state=random_state)\n",
    "\n",
    "        train_df = dataframe[dataframe[\"image\"].isin(train_images)]\n",
    "        val_df = dataframe[dataframe[\"image\"].isin(val_images)]\n",
    "        test_df = dataframe[dataframe[\"image\"].isin(test_images)]\n",
    "\n",
    "        if get_len:\n",
    "            print(\"Overview of length after split:\")\n",
    "            print(\"Number of unique images:\", len(unique_images))\n",
    "            print(f\"Train dataset contains {len(train_df)} items.\")\n",
    "            print(f\"Validation dataset contains {len(val_df)} items.\")\n",
    "            print(f\"Test dataset contains {len(test_df)} items.\")\n",
    "\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def create_datasets(self, dataframe, get_len=False):\n",
    "        train_df, val_df, test_df = self.split_dataframe_by_images(dataframe, get_len=get_len)\n",
    "        train_dataset = ImageCaptionDataset(train_df, self.image_path, self.vocab_size, self.caption_info, self.image_transformations)\n",
    "        val_dataset = ImageCaptionDataset(val_df, self.image_path, self.vocab_size, self.caption_info, self.image_transformations)\n",
    "        test_dataset = ImageCaptionDataset(test_df, self.image_path, self.vocab_size, self.caption_info, self.image_transformations)\n",
    "\n",
    "        if get_len:\n",
    "            train_len, val_len, test_len = len(train_dataset), len(val_dataset), len(test_dataset)\n",
    "            print(\"Overview of items in Dataset:\")\n",
    "            print(f\"Train dataset contains {train_len} items.\")\n",
    "            print(f\"Validation dataset contains {val_len} items.\")\n",
    "            print(f\"Test dataset contains {test_len} items.\")\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def create_data_loaders(self, dataframe, get_len=False):\n",
    "\n",
    "        train_dataset, val_dataset, test_dataset = self.create_datasets(dataframe, get_len=get_len)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, pin_memory=False, num_workers=0, persistent_workers=False, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, pin_memory=False, num_workers=0, persistent_workers=False, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, pin_memory=False, num_workers=0, persistent_workers=False, shuffle=False)\n",
    "\n",
    "        if get_len:\n",
    "            train_len, val_len, test_len = len(train_loader), len(val_loader), len(test_loader)\n",
    "            print(\"Overview of batches:\")\n",
    "            print(f\"Number of batches in train_loader: {train_len}\")\n",
    "            print(f\"Number of batches in val_loader: {val_len}\")\n",
    "            print(f\"Number of batches in test_loader: {test_len}\")\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Test Image Caption Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ImageCaptionDataset\n",
    "dataframe = flicker_data\n",
    "image_path = image_path\n",
    "vocab_size = vocab_size\n",
    "caption_processor = caption_info\n",
    "transform = None\n",
    "max_length = 20\n",
    "\n",
    "dataset = ImageCaptionDataset(dataframe, image_path, vocab_size, caption_processor, transform, max_length)\n",
    "print(\"Dataset Size:\", len(dataset))\n",
    "\n",
    "# Get a sample from the dataset\n",
    "image, caption = dataset[0]\n",
    "print(\"Image:\", image)\n",
    "print(\"Caption:\", caption)\n",
    "print(\"Caption Size:\", caption.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation = DataPreparation(image_path, vocab_size, caption_info, image_transformations, batch_size=64)\n",
    "data_preparation_augmented = DataPreparation(image_path, vocab_size, caption_info, image_transforms_augmented, batch_size=64)\n",
    "\n",
    "# Test DataPreparation.split_dataframe_by_images\n",
    "train_df, val_df, test_df = data_preparation.split_dataframe_by_images(flicker_data, get_len=False)\n",
    "\n",
    "# Test DataPreparation.create_datasets\n",
    "train_dataset, val_dataset, test_dataset = data_preparation.create_datasets(flicker_data, get_len=False)\n",
    "\n",
    "# Test DataPreparation.create_data_loaders\n",
    "train_loader, val_loader, test_loader = data_preparation.create_data_loaders(flicker_data, get_len=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = data_preparation.create_data_loaders(flicker_data, get_len=False)\n",
    "\n",
    "train_loader_augmented, val_loader_augmented, test_loader_augmented = data_preparation_augmented.create_data_loaders(flicker_data, get_len=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first batch form the train_loader\n",
    "images, captions = next(iter(train_loader))\n",
    "print(\"Images Size:\", images.size())\n",
    "print(\"Captions Size:\", captions.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 6 CNN Encoder\n",
    "\n",
    "Der `EncoderCNN` ist ein PyTorch Lightning-Modul, das für die Aufgabe des \"Zeigen und Erzählen\" entwickelt wurde, eine häufige Problemstellung im Bereich der Bildunterschriftenerzeugung. Seine Hauptaufgabe besteht darin, Merkmale aus Bildern zu extrahieren, die von einem Textgenerator (Decoder) verwendet werden können, um eine Beschreibung oder einen Text für das Bild zu generieren.\n",
    "\n",
    "Konstruktor (`__init__`-Methode):\n",
    "- Der Konstruktor initialisiert den Encoder und nimmt `embedding_dim` als Parameter entgegen, der die Dimension der aus den Bildern extrahierten Merkmale festlegt.\n",
    "- Er verwendet das ResNet-18-Modell aus der PyTorch-Modellebibliothek, das mit vorab trainierten Gewichten aus dem ImageNet-1K-Datensatz initialisiert wird.\n",
    "- Alle Parameter des ResNet-18, außer den Parametern des letzten linearen Layers, werden eingefroren, um sicherzustellen, dass diese Gewichtungen während des Trainings nicht aktualisiert werden.\n",
    "- Der letzte lineare Layer des ResNet-18-Modells wird durch einen neuen Sequential-Block ersetzt, der aus einer linearen Schicht (`nn.Linear`) gefolgt von einer Batch-Normalisierungsschicht (`nn.BatchNorm1d`) besteht. Dieser Block dient dazu, die Ausgabe des Modells auf die gewünschte Embedding-Dimension zu reduzieren und die Konvergenz des Modells zu verbessern.\n",
    "\n",
    "`forward`-Methode:\n",
    "- Diese Methode ist für die Vorwärtsdurchlaufoperation verantwortlich. Sie nimmt eine Eingabe `images` in Form von Bildern und einen optionalen Parameter `print_dimensions`, der zum Drucken der Dimensionen der Zwischenergebnisse verwendet werden kann.\n",
    "- Die Methode leitet die Eingabe durch das ResNet-18-Modell und gibt die extrahierten Merkmale (Embeddings) zurück.\n",
    "- Wenn `print_dimensions` auf `True` gesetzt ist, wird die Größe der Eingabebilder vor der Weitergabe an das Modell gedruckt.\n",
    "\n",
    "Insgesamt bietet dieser `EncoderCNN` eine effektive Möglichkeit, Bildmerkmale für die Verwendung in einem Textgenerator (Decoder) zur Erzeugung von Bildbeschreibungen zu extrahieren. Die Verwendung eines vorab trainierten ResNet-18-Modells mit eingefrorenen Gewichtungen und einer angepassten Ausgabeschicht trägt zur Verbesserung der Genauigkeit und Effizienz des Modells bei.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Encoder Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim # define embedding dimension\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # Use ResNet-18 with pre-trained weights IMAGENET1K_V1) equivalen tto Defualt\n",
    "\n",
    "        # freeze all layers except the last linear layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # overwrite the last layer\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, self.embedding_dim), \n",
    "                                       nn.BatchNorm1d(self.embedding_dim, momentum=0.01))\n",
    "\n",
    "    def forward(self, images, print_dimensions=False):\n",
    "        # extract features from the images\n",
    "        features = self.resnet(images)\n",
    "        if print_dimensions:\n",
    "            print(\"Images Size:\", images.size())\n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Encoder DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDenseNet(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim # define embedding dimension\n",
    "        self.densenet = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)  # Use DenseNet-121 with pre-trained weights\n",
    "\n",
    "        # Modify the classifier to match the embedding dimension\n",
    "        num_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, self.embedding_dim),\n",
    "            nn.BatchNorm1d(self.embedding_dim, momentum=0.01))\n",
    "\n",
    "    def forward(self, images, print_dimensions=False):\n",
    "        # Extract features from the images\n",
    "        features = self.densenet(images)\n",
    "        if print_dimensions:\n",
    "            print(\"Images Size:\", images.size())\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Test Encoder Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize encoder\n",
    "encoder = EncoderCNN(embedding_dim=128)\n",
    "#encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Test Resnet18 Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features with forward\n",
    "features = encoder.forward(images, print_dimensions=True)\n",
    "print(features.size())\n",
    "print(captions.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Test Encoder DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder2 = EncoderDenseNet(embedding_dim=128)\n",
    "#encoder2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Test DenseNet121 Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features with forward\n",
    "features = encoder2.forward(images, print_dimensions=True)\n",
    "print(features.size())\n",
    "print(captions.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7 LSTM Decoder\n",
    "\n",
    "DecoderLSTM Klasse\n",
    "\n",
    "Die `DecoderLSTM` Klasse ist ein PyTorch Lightning-Modul, das in einer Show & Tell-Implementierung verwendet wird, um Bildunterschriften zu generieren. Diese Klasse dient als Decoder und erzeugt Textbasierend auf den Merkmalen des Bildes und den bisher generierten Teilen der Bildunterschrift. Hier ist eine Übersicht über die wichtigsten Eigenschaften und Methoden der Klasse:\n",
    "\n",
    "Der Konstruktor `__init__` der `DecoderLSTM` Klasse wird beim Erstellen eines Decoder-Objekts aufgerufen. Er initialisiert die wichtigsten Eigenschaften und Schichten des Decoders. Hier sind die Parameter und ihre Bedeutung:\n",
    "\n",
    "`embedding_dim`: Die Dimension der Wortvektoren in der Einbettungsschicht.  \n",
    "`hidden_size`: Die Anzahl der versteckten Einheiten in der LSTM-Schicht.  \n",
    "`vocab_size`: Die Größe des Vokabulars, d.h. die Anzahl der möglichen Wörter im Ausgabevokabular.  \n",
    "`num_layers`: Die Anzahl der LSTM-Schichten in der Decoder-Architektur (Standardwert: 1).  \n",
    "\n",
    "Im Konstruktor werden diese Parameter als Eigenschaften der Klasse gespeichert, um auf sie in anderen Methoden zugreifen zu können. Darüber hinaus werden die folgenden Schichten initialisiert:\n",
    "\n",
    "`self.embedding`: Eine Einbettungsschicht (Embedding Layer), die verwendet wird, um Wörter in Vektoren umzuwandeln.  \n",
    "`self.lstm`: Eine LSTM-Schicht, die die Hauptkomponente des Decoders darstellt und die Sequenzgenerierung ermöglicht.  \n",
    "`self.linear`: Eine lineare Schicht, die die Ausgabe des LSTMs auf die Dimension des Vokabulars abbildet, um die vorhergesagten Wortverteilungen zu erzeugen.  \n",
    "\n",
    "Methoden\n",
    "\n",
    "`forward(self, features, captions, print_dimensions=False)`\n",
    "\n",
    "Diese Methode führt einen Vorwärtsdurchlauf durch den Decoder durch. Sie nimmt Bildmerkmale (`features`) und bisher generierte Untertitel (`captions`) entgegen und gibt die vorhergesagten Wortverteilungen für jeden Zeitschritt zurück. Bei Bedarf können die Größen der Zwischenschritte gedruckt werden.\n",
    "\n",
    "`greedy_sample(self, features, max_length=20, print_dimensions=False)`\n",
    "\n",
    "Diese Methode verwendet eine Greedy-Sampling-Strategie, um einen Bildunterschrift-Text zu generieren. Sie nimmt Bildmerkmale (`features`) und eine maximale Ausgabelänge (`max_length`) entgegen und gibt den generierten Text zurück. Diese Methode iteriert durch die Zeitschritte und wählt bei jedem Schritt das wahrscheinlichste Wort aus. Sie ist nützlich für die Generierung von Bildunterschriften mit einer festen Länge. Bei Bedarf können die Größen der Zwischenschritte gedruckt werden.\n",
    "\n",
    "Die `DecoderLSTM` Klasse ermöglicht es, den Decoder in einer Show & Tell-Bildunterschriften-Implementierung zu nutzen, um Bildbeschreibungen zu generieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "\n",
    "        # define the properties\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define the layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, features, captions, print_dimensions=False):\n",
    "        # Embed the captions\n",
    "        embeddings = self.embedding(captions)\n",
    "        # Concatenate the feature vectors and embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        # Pass the embeddings through the LSTM cells\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        # Pass the outputs through the linear layer\n",
    "        outputs = self.linear(hiddens)\n",
    "        # slice output to remove the last time step\n",
    "        outputs = outputs[:, :-1, :]\n",
    "\n",
    "        if print_dimensions:\n",
    "            print(\"Embeddings Size:\", embeddings.size())\n",
    "            print(\"Hiddens Size:\", hiddens.size())\n",
    "            print(\"Outputs Size:\", outputs.size())\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def greedy_sample(self, features, max_length=20, print_dimensions=False):\n",
    "        # add a dimension to the features tensor to match the captions tensor\n",
    "        features = features.unsqueeze(1)\n",
    "        # initialize the output tensor\n",
    "        token_output = torch.zeros((features.size(0), max_length)).to(features.device)\n",
    "        hidden = None\n",
    "\n",
    "        # Prepare the initial input for LSTM, which is the features tensor\n",
    "        for i in range(max_length):\n",
    "            # Pass the features through the LSTM cells\n",
    "            lstm_output, _ = self.lstm(features)  \n",
    "            # Pass the LSTM outputs through the linear layer\n",
    "            outputs = self.linear(lstm_output) \n",
    "            # Get the outputs for the last step\n",
    "            outputs = outputs[:, -1, :].unsqueeze(1)  \n",
    "            # Get the predicted word indices\n",
    "            predicted = outputs.argmax(dim=2)  \n",
    "            # Save the predicted word index in a tensor\n",
    "            token_output[:, i] = predicted.squeeze(1) \n",
    "            # Embed the predicted word index / Prepare the input for the next step\n",
    "            predicted = self.embedding(predicted)  \n",
    "            # concatenate the feature vectors and embeddings\n",
    "            features = torch.cat((features, predicted), dim=1) \n",
    "        \n",
    "        if print_dimensions:\n",
    "            print(\"Features Size:\", features.size())\n",
    "            print(\"lstm_output Size:\", lstm_output.size())\n",
    "            print(\"outputs Size after Linear Layer:\", outputs.size())\n",
    "            print(\"Predicted Size:\", predicted.size())\n",
    "            print(\"Token Output Size:\", token_output.size())\n",
    "\n",
    "        return token_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Test LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize decoder\n",
    "decoder = DecoderLSTM(embedding_dim=128, hidden_size=128, vocab_size=vocab_size)\n",
    "#decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Test Decoder Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the decoder outputs\n",
    "outputs = decoder(features, captions)\n",
    "print(\"Outputs Size:\", outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Test Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the greedy sampled outputs\n",
    "sampled_ids = decoder.greedy_sample(features, print_dimensions=True)\n",
    "sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use processCaption to convert caption to tokens\n",
    "print(caption_info.tokens_to_caption(sampled_ids[0].tolist()))\n",
    "print(caption_info.tokens_to_caption(sampled_ids[1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8 Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Resnet18 Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel1(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=None, val_loader=None, learning_rate=0.001, weight_decay=0):\n",
    "        super().__init__()\n",
    "        # Initialize the encoder and decoder\n",
    "        self.encoder = EncoderCNN(embedding_dim)\n",
    "        self.decoder = DecoderLSTM(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "        # Data loaders\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader    \n",
    "\n",
    "        # Hyperparameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # Pass the images through the encoder\n",
    "        features = self.encoder.forward(images)\n",
    "        # Pass the features and captions through the decoder\n",
    "        outputs = self.decoder.forward(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def generate_token_caption(self, image, max_length=20):\n",
    "        # Process the image through the encoder to get the features\n",
    "        features = self.encoder.forward(image)\n",
    "        # Generate caption using the decoder's greedy sampling method\n",
    "        caption = self.decoder.greedy_sample(features, max_length=max_length)\n",
    "        return caption\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self.forward(images, captions[:, :-1])\n",
    "        loss = F.cross_entropy(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self.forward(images, captions[:, :-1])\n",
    "        loss = F.cross_entropy(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(params=self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 DenseNet121 Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel2(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=None, val_loader=None, learning_rate=0.001, weight_decay=0):\n",
    "        super().__init__()\n",
    "        # Initialize the encoder and decoder\n",
    "        self.encoder = EncoderDenseNet(embedding_dim)\n",
    "        self.decoder = DecoderLSTM(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "        # Data loaders\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader    \n",
    "\n",
    "        # Hyperparameters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        # Pass the images through the encoder\n",
    "        features = self.encoder.forward(images)\n",
    "        # Pass the features and captions through the decoder\n",
    "        outputs = self.decoder.forward(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def generate_token_caption(self, image, max_length=20):\n",
    "        # Process the image through the encoder to get the features\n",
    "        features = self.encoder.forward(image)\n",
    "        # Generate caption using the decoder's greedy sampling method\n",
    "        caption = self.decoder.greedy_sample(features, max_length=max_length)\n",
    "        return caption\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self.forward(images, captions[:, :-1])\n",
    "        loss = F.cross_entropy(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self.forward(images, captions[:, :-1])\n",
    "        loss = F.cross_entropy(outputs.reshape(-1, outputs.size(2)), captions[:, 1:].reshape(-1))\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(params=self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Test Resnet18 Encoder-Decoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nic_model = ImageCaptioningModel1(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader, val_loader=val_loader, learning_rate=0.001, weight_decay=0)\n",
    "#nic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Test Resnet18 Encoder-Decoder Generate Caption before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model outputs\n",
    "outputs = nic_model.forward(images, captions)\n",
    "print(\"Outputs Size:\", outputs.size())\n",
    "# get the greedy sampled outputs\n",
    "sampled_ids = nic_model.generate_token_caption(images)\n",
    "# convert the sampled_ids to captions\n",
    "print(caption_info.tokens_to_caption(sampled_ids[0].tolist()))\n",
    "print(caption_info.tokens_to_caption(sampled_ids[1].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Test DenseNet121 Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nic_model2 = ImageCaptioningModel2(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader, val_loader=val_loader, learning_rate=0.001, weight_decay=0)\n",
    "#nic_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Test Densenet121 Encoder-Decoder Generate Caption before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = nic_model2.forward(images, captions)\n",
    "print(\"Outputs Size:\", outputs.size())\n",
    "# get the greedy sampled outputs\n",
    "sampled_ids = nic_model2.generate_token_caption(images)\n",
    "# convert the sampled_ids to captions\n",
    "print(caption_info.tokens_to_caption(sampled_ids[0].tolist()))\n",
    "print(caption_info.tokens_to_caption(sampled_ids[1].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbTraining:\n",
    "    def __init__(self, model, train_loader, val_loader, sweep_config, project_name, entity_name, max_epochs_train=100, max_epochs_sweep=30):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.sweep_config = sweep_config\n",
    "        self.project_name = project_name\n",
    "        self.entity_name = entity_name\n",
    "        self.max_epochs_train = max_epochs_train\n",
    "        self.max_epochs_sweep = max_epochs_sweep\n",
    "\n",
    "    def _define_name_model(self):\n",
    "        filename = f\"batch64-lr{self.model.learning_rate}-emb{self.model.embedding_dim}-hid{self.model.hidden_size}-layers{self.model.num_layers}-wd{self.model.weight_decay}\"\n",
    "        return filename\n",
    "    \n",
    "    def _define_name_sweep(self, config):\n",
    "        filename = f\"batch64-lr{config.learning_rate}-emb{config.embedding_dim}-hid{config.hidden_size}-layers{config.num_layers}-wd{config.weight_decay}\"\n",
    "        return filename\n",
    "\n",
    "    def _wandb_logger(self):\n",
    "        filename = self._define_name_model()\n",
    "        wandb_logger = WandbLogger(project=self.project_name, entity=self.entity_name, name=filename)\n",
    "        return wandb_logger\n",
    "\n",
    "    def wandb_training(self, checkpointpath=\"checkpoints/\"):\n",
    "        \n",
    "        filename = self._define_name_model()\n",
    "        wandb_logger = self._wandb_logger()\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=self.max_epochs_train, \n",
    "            logger=wandb_logger, \n",
    "            accelerator=\"gpu\", \n",
    "            callbacks=[\n",
    "                ModelCheckpoint(monitor='val_loss', mode='min', dirpath=checkpointpath, filename=filename, save_top_k=1),\n",
    "                EarlyStopping(monitor='val_loss', mode='min')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.fit(self.model, self.train_loader, self.val_loader)\n",
    "        wandb.finish()\n",
    "\n",
    "    def sweep_training(self, checkpointpath=\"checkpoints/\"):\n",
    "        sweep_id = wandb.sweep(self.sweep_config, project=self.project_name, entity=self.entity_name)\n",
    "\n",
    "        def train():\n",
    "            wandb_init = wandb.init(project=self.project_name, entity=self.entity_name)\n",
    "            config = wandb_init.config\n",
    "\n",
    "            model = ImageCaptioningModel1(embedding_dim=config.embedding_dim, \n",
    "                                 hidden_size=config.hidden_size, \n",
    "                                 vocab_size=vocab_size, \n",
    "                                 train_loader=train_loader, \n",
    "                                 val_loader=val_loader, \n",
    "                                 learning_rate=config.learning_rate, \n",
    "                                 num_layers=config.num_layers,\n",
    "                                 weight_decay=config.weight_decay)\n",
    "\n",
    "            filename = self._define_name_sweep(config)\n",
    "            wandb_logger = self._wandb_logger()\n",
    "\n",
    "            trainer = Trainer(\n",
    "                max_epochs=self.max_epochs_sweep, \n",
    "                logger=wandb_logger, \n",
    "                accelerator=\"cuda\",\n",
    "                devices=\"auto\", \n",
    "                callbacks=[\n",
    "                    ModelCheckpoint(monitor='val_loss', mode='min', dirpath=checkpointpath, filename=filename, save_top_k=1),\n",
    "                    EarlyStopping(monitor='val_loss', patience=3, mode='min') # patience=3 default value\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            trainer.fit(model, self.train_loader, self.val_loader)\n",
    "            wandb.finish()\n",
    "\n",
    "        wandb.agent(sweep_id, train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sweep config\n",
    "sweep_config_resnet = {\n",
    "    'method': 'grid', # grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [256, 512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [128, 256, 512]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001, 0.005]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call class\n",
    "wandb_trainer_nic1 = WandbTraining(nic_model, train_loader, val_loader, sweep_config_resnet, project_name=\"del-mc2\", entity_name=\"7ben18\", max_epochs_train=10, max_epochs_sweep=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Resnet18 Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test method wandb_training\n",
    "#wandb_trainer_nic1.wandb_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Resnet18 Model Training\n",
    "\n",
    "Für alle Nachfolgende Modell Training wurden folgende Hyperparameter fixiert und unverändert gelassen:\n",
    "\n",
    "| Hyperparameter | Wert | Beschreibung |\n",
    "| --- | --- | --- |\n",
    "| vocab_size | 8375 | Grösse des Vokabulars änderbar durch Thresholding in der Klasse `PreProcessing`. <br> Siehe Abschnitt 4.1 für Initialisierung.|\n",
    "| batch_size | 64 | Anzahl der Bilder pro Batch, durch Klasse `DataPreparation` definiert. <br> Siehe Abschnitt 5.2 für Initialisierung. |\n",
    "| num_layers | 1 | Anzahl der LSTM Schichten im Decoder, durch Klasse `DecoderLSTM` definiert. <br> Siehe Abschnitt 7.1 für Initialisierung. |\n",
    "| epoche | 30 | Anzhal Epoche die ein Run durchläuft, durch Klasse `WandbTrainer` definiert. <br> Siehe Abschnitt 9 für Initialisierung. |\n",
    "\n",
    "Tabellenübersicht mit Sweep Runs:\n",
    "\n",
    "| Sweep ID | Link | Beschreibung |\n",
    "| --- | --- | --- | \n",
    "| saigdbbn | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/saigdbbn?workspace=user-7ben18) | Test Sweep | \n",
    "| u94ihcuc | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/u94ihcuc?workspace=user-7ben18) | Test Sweep |\n",
    "| 8gvjrmgj | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/8gvjrmgj?workspace=user-7ben18) | Test Sweep |\n",
    "| szoouw9b | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/szoouw9b?workspace=user-7ben18) | Erster Sweep Durchgang mit <br> unterschiedlichen Hyperparameter Grid Search | \n",
    "| dr9040xi | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/dr9040xi?workspace=user-7ben18) | Zweiter Sweep Durchgang mit <br> angepassten Hyperparameter Grid Search |\n",
    "\n",
    "Resutlat:\n",
    "Aus den Sweep Runs wurden folgende Hyperparameter für das Training des Modells ausgewählt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Sweep training\n",
    "\n",
    "#wandb_trainer_nic1.sweep_training(checkpointpath=\"checkpoints/nic1-normal/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Resnet18 Regularisierung\n",
    "\n",
    "Wir fixieren den Hyperparameter Leraning Rate und führen nun einen Sweep mit Regularisierung durch.\n",
    "\n",
    "Tabellenübersicht mit Sweep Runs:\n",
    "| Sweep ID | Link | Beschreibung | Bemerkung | \n",
    "| --- | --- | --- | --- |\n",
    "| 2854z20q | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/2854z20q?workspace=user-7ben18) | Sweep mit Regularisierung <br> `[0.1, 0.01, 0.001]` | Die Regularisierungsparameter wurden zu hoch gewählt, <br> im nächsten Sweep fixieren wir die Hyperparameter <br> embedding_dim und hidden_size auf 512, wie es beim Paper beschrieben worden ist <br> und erweitern die maximale Epoche für einen Sweep Run auf 50.|\n",
    "| pq27vp2c | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/pq27vp2c?workspace=user-7ben18) | Sweep mit Regularisierung <br> `[0.0001, 0.00001, 0.000001]` | Folgende Hyperparameter fixiert im Sweep_Config: <br> embedding_dim=512, hidden_size=512, learning_rate=0.001 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nic_model_reg = ImageCaptioningModel1(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader, val_loader=val_loader, learning_rate=0.001, weight_decay=0.001)\n",
    "\n",
    "# define sweep config\n",
    "sweep_config_resnet_reg = {\n",
    "    'method': 'grid', # grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [512] # 512 was used in the paper\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [512] # 512 was used in the paper\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0001, 0.00001, 0.000001]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# initialize wandb training\n",
    "wandb_trainer_nic1_reg = WandbTraining(nic_model_reg, train_loader, val_loader, sweep_config_resnet_reg, project_name=\"del-mc2\", entity_name=\"7ben18\", max_epochs_sweep=50)\n",
    "\n",
    "# Sweep Run\n",
    "#wandb_trainer_nic1_reg.sweep_training(checkpointpath=\"checkpoints/nic1-reg/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 DenseNet121 Model Training\n",
    "\n",
    "Analog wie beim vorherhigen Abschnitt 9.2 Resnet18 Model Training fixieren wir die gleichen Hyperparameter und führen einen Sweep durch.\n",
    "\n",
    "Tabellenübersicht mit Sweep Runs:\n",
    "\n",
    "| Sweep ID | Link | Beschreibung |\n",
    "| --- | --- | --- |\n",
    "| 6rvz7beu | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/6rvz7beu?workspace=user-7ben18) | Erster Sweep Druchgang mit <br> unterschiedlichen Hyperaparameter Grid Search |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model (ImageCaptionModel2)\n",
    "nic_model2 = ImageCaptioningModel2(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader, val_loader=val_loader, learning_rate=0.001, weight_decay=0)\n",
    "\n",
    "# define sweep config\n",
    "sweep_config_densenet = {\n",
    "    'method': 'grid', # grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001, 0.005]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize wandb training\n",
    "wandb_trainer_nic2 = WandbTraining(nic_model2, train_loader, val_loader, sweep_config_densenet, project_name=\"del-mc2\", entity_name=\"7ben18\", max_epochs_train=10, max_epochs_sweep=30)\n",
    "\n",
    "#wandb_trainer_nic2.sweep_training(checkpointpath=\"checkpoints/nic2-normal/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 DenseNet121 Regularisierung\n",
    "\n",
    "Wir fixieren den Hyperparameter Leraning Rate und führen nun einen Sweep mit Regularisierung durch.\n",
    "\n",
    "Tabellenübersicht mit Sweep Runs:\n",
    "\n",
    "| Sweep ID | Link | Beschreibung |\n",
    "| --- | --- | --- |\n",
    "| u409aqw2 | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/u409aqw2?workspace=user-7ben18) | Sweep mit Regularisierung <br> `0.0001, 0.00001, 0.0000001` | Die Regularisierungsparameter wurden zu hoch gewählt, <br> im nächsten Sweep fixieren wir die Hyperparameter <br> embedding_dim und hidden_size auf 512, wie es beim Paper beschrieben worden ist <br> und erweitern die maximale Epoche für einen Sweep Run auf 50.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model (ImageCaptionModel2)\n",
    "nic_model2_reg = ImageCaptioningModel2(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader, val_loader=val_loader, learning_rate=0.001, weight_decay=0.001)\n",
    "\n",
    "\n",
    "# define sweep config\n",
    "sweep_config_densnet_reg = {\n",
    "    'method': 'grid', # grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [512] # 512 was used in the paper\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [512]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0001, 0.00001, 0.000001]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# initialize wandb training\n",
    "wandb_trainer_nic2_reg = WandbTraining(nic_model2_reg, train_loader, val_loader, sweep_config_densnet_reg, project_name=\"del-mc2\", entity_name=\"7ben18\", max_epochs_train=10, max_epochs_sweep=30)\n",
    "\n",
    "# Sweep Run\n",
    "#wandb_trainer_nic2_reg.sweep_training(checkpointpath=\"checkpoints/nic2-reg/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Resnet18 Model Training mit Augmentation\n",
    "\n",
    "Wir versuchen mittels Data Augmentation die Performance des Modells zu verbessern.\n",
    "\n",
    "Tabellenübersicht mit Sweep Runs:\n",
    "| Sweep ID | Link | Beschreibung |\n",
    "| --- | --- | --- |\n",
    "| p5wg8g6v | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/p5wg8g6v?workspace=user-7ben18) | Erster Sweep Druchgang mit <br> unterschiedlichen Hyperaparameter Grid Search |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nic_model_aug = ImageCaptioningModel1(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader_augmented, val_loader=val_loader_augmented, learning_rate=0.001, weight_decay=0.001)\n",
    "\n",
    "# define sweep config\n",
    "sweep_config_resnet_aug = {\n",
    "    'method': 'grid', # grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0001, 0.00001, 0.000001]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# initialize wandb training\n",
    "wandb_trainer_nic1_aug = WandbTraining(nic_model_aug, train_loader_augmented, val_loader_augmented, sweep_config_resnet_aug, project_name=\"del-mc2\", entity_name=\"7ben18\", max_epochs_sweep=50)\n",
    "\n",
    "# Sweep Run\n",
    "wandb_trainer_nic1_aug.sweep_training(checkpointpath=\"checkpoints/nic1-aug/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 DenseNet121 Model Training mit Augmentation\n",
    "\n",
    "Wir versuchen mittels Data Augmentation die Performance des Modells zu verbessern.\n",
    "\n",
    "Tabellenübersicht mit Sweep Runs:\n",
    "| Sweep ID | Link | Beschreibung |\n",
    "| --- | --- | --- |\n",
    "| pmyg0xvp | [Sweep Overview](https://wandb.ai/7ben18/del-mc2/sweeps/pmyg0xvp?workspace=user-7ben18) | Erster Sweep Druchgang mit <br> unterschiedlichen Hyperaparameter Grid Search |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nic_model2_aug = ImageCaptioningModel2(vocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader_augmented, val_loader=val_loader_augmented, learning_rate=0.001, weight_decay=0.001)\n",
    "\n",
    "# define sweep config\n",
    "sweep_config_densenet_aug = {\n",
    "    'method': 'grid', # grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': {\n",
    "            'values': [512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [512, 1024] # 512 was used in the paper\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.001]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.0001, 0.00001, 0.000001]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# initialize wandb training\n",
    "wandb_trainer_nic2_aug = WandbTraining(nic_model2_aug, train_loader_augmented, val_loader_augmented, sweep_config_densenet_aug, project_name=\"del-mc2\", entity_name=\"7ben18\", max_epochs_sweep=50)\n",
    "\n",
    "# Sweep Run\n",
    "wandb_trainer_nic2_aug.sweep_training(checkpointpath=\"checkpoints/nic2-aug/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Load Best Model von Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_checkpoint_path = 'checkpoints/batch64-lr0.001-emb512-hid128-layers1.ckpt'\n",
    "best_nic_model = ImageCaptioningModel1.load_from_checkpoint(best_model_checkpoint_path, evocab_size=vocab_size, embedding_dim=512, hidden_size=128, num_layers=1, train_loader=train_loader, val_loader=val_loader, learning_rate=0.001, weight_decay=0.001)\n",
    "# Move to gpu\n",
    "best_nic_model = best_nic_model.to(device_setup.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Evaluation - BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Bleu Score\n",
    "\n",
    "Die Evaluation des Modells erfolgt mit dem BLEU Score. Der BLEU Score ist ein Mass für die Qualität von Textgenerierung. Er vergleicht die generierten Texte mit den Referenztexten.\n",
    "\n",
    "BLEU Score für Generierte Sätze\n",
    "\n",
    "Der **BLEU Score (Bilingual Evaluation Understudy)** ist eine Metrik, die ursprünglich für die Bewertung maschineller Übersetzungen entwickelt wurde, aber auch auf generierte Sätze in Bereichen wie Textgenerierung und Chatbots angewendet werden kann.\n",
    "\n",
    "Grundkonzept\n",
    "\n",
    "Der BLEU-Score misst, wie nah generierte Sätze an eine oder mehrere Referenzsätze herankommen, die von Menschen erstellt wurden. Ein höherer BLEU-Score deutet darauf hin, dass die generierten Sätze den Referenzsätzen ähnlicher sind.\n",
    "\n",
    "Komponenten des BLEU-Scores\n",
    "\n",
    "1. **N-Gram-Übereinstimmung**: BLEU betrachtet die Übereinstimmungen von N-Grammen (Sequenzen von N Wörtern) zwischen dem generierten Satz und den Referenzsätzen.\n",
    "2. **Strafe für zu kurze Sätze (Brevity Penalty, BP)**: Wenn der generierte Satz kürzer ist als die Referenzsätze, wird eine Strafe verhängt, um das Generieren von zu kurzen Sätzen zu vermeiden.\n",
    "\n",
    "Mathematische Formel des BLEU-Scores\n",
    "\n",
    "$\\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)$\n",
    "\n",
    "\n",
    "Der BLEU-Score wird folgendermaßen berechnet:\n",
    "\n",
    "\n",
    "- `p_n` ist die Präzision der N-Gramme.\n",
    "- `w_n` sind Gewichte für die Präzision der verschiedenen N-Gramme.\n",
    "- `BP` ist die Brevity Penalty, berechnet als:\n",
    "\n",
    "\n",
    "Hierbei ist `c` die Länge des generierten Satzes und `r` die Länge der am nächsten liegenden Referenz.\n",
    "\n",
    "\n",
    "Beispiel\n",
    "\n",
    "Nehmen wir an, wir haben folgende Sätze:\n",
    "\n",
    "- Generierter Satz: \"Das Haus ist klein\"\n",
    "- Referenzsatz: \"Das kleine Haus\"\n",
    "\n",
    "Der 1-Gramm Score (`p_1`) wäre der Anteil der Wörter im generierten Satz, die auch im Referenzsatz vorkommen. Die Brevity Penalty würde angewendet, da der generierte Satz länger ist.\n",
    "\n",
    "\n",
    "Schlussfolgerung\n",
    "\n",
    "Der BLEU-Score ist ein nützliches Werkzeug zur Bewertung der Qualität maschineller Übersetzungen, hat aber auch seine Grenzen. Er kann nicht die Flüssigkeit oder die grammatikalische Richtigkeit vollständig erfassen und sollte daher als Teil eines umfassenderen Bewertungsansatzes verwendet werden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_score(test_dataset, model):\n",
    "    \"\"\"\n",
    "    This function calculates the BLEU score for the model.\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from tqdm import tqdm\n",
    "    import torch\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # create empty dataframe to store image, bleu, caption and generated caption\n",
    "    bleu_df = pd.DataFrame(columns=['image', 'bleu', 'caption', 'generated_caption'])\n",
    "\n",
    "    # Initialize the lists to store references and hypotheses\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    for idx in tqdm(range(0, len(test_dataset), 5)):\n",
    "        # Get the image and caption\n",
    "        image, caption = test_dataset[idx]\n",
    "        # Convert the image to a tensor\n",
    "        image = image.unsqueeze(0)\n",
    "        # Generate a caption using the model\n",
    "        generated_caption = model.generate_token_caption(image)\n",
    "        # Convert the generated caption tensor to a list of integers\n",
    "        generated_caption = generated_caption.squeeze(0).tolist()\n",
    "        # Convert the integers to a caption using the vocabulary\n",
    "        generated_caption = caption_info.tokens_to_caption(generated_caption)\n",
    "        # Store the reference and hypothesis\n",
    "        references.append([caption])\n",
    "        hypotheses.append(generated_caption)\n",
    "        # Create a dataframe to store the results and concat then with the bleu_df\n",
    "        df = pd.DataFrame({'image': [idx], 'bleu': [corpus_bleu(references, hypotheses)], 'caption': [caption], 'generated_caption': [generated_caption]})\n",
    "        bleu_df = pd.concat([bleu_df, df], ignore_index=True)\n",
    "\n",
    "    # sort the dataframe by highest bleu score\n",
    "    bleu_df = bleu_df.sort_values(by='bleu', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Calculate the BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    return bleu_df, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bleu score\n",
    "bleu_df, bleu_score = blue_score(test_dataset, nic_model)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "display(bleu_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Visualisierung Image Captioning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
